{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Navigation with DQN - Tuned\n",
    "\n",
    "\n",
    "This is a bonus task where this DQN is tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "env = gameEnv(partial=False,size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168]) # 84 x 84 x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions allow us to update the parameters of our target network with those of the primary network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        #We use slim.conv2d to set up our network \n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        ################################################################################\n",
    "        # TODO: Implement Dueling DQN                                                  #\n",
    "        # We take the output from the final convolutional layer i.e. self.conv4 and    #\n",
    "        # split it into separate advantage and value streams.                          #\n",
    "        # Outout: self.Advantage, self.Value                                           #\n",
    "        # Hint: Refer to Fig.1 in [Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)  #\n",
    "        #       In implementation, use tf.split to split into two branches. You may    #\n",
    "        #       use xavier_initializer for initializing the two additional linear      #\n",
    "        #       layers.                                                                # \n",
    "        ################################################################################\n",
    "        adv, val = tf.split(self.conv4, 2, 3)\n",
    "        self.Advantage = tf.layers.dense(slim.flatten(adv), env.actions)\n",
    "        self.Value = tf.layers.dense(slim.flatten(val), 1)\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        \n",
    "        #Then combine them together to get our final Q-values. \n",
    "        #Please refer to Equation (9) in [Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Obtain the loss (self.loss) by taking the sum of squares difference          #\n",
    "        # between the target and prediction Q values.                                  #\n",
    "        ################################################################################\n",
    "        predictQ = tf.reduce_sum(self.Qout*self.actions_onehot,axis = 1)\n",
    "        self.loss = tf.reduce_mean((predictQ - self.targetQ)**2)\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting all the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 5000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 100 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-a539cef2bc60>:36: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Episode 9 reward: 2.8\n",
      "Episode 19 reward: 4.4\n",
      "Episode 29 reward: 2.8\n",
      "Episode 39 reward: 3.3\n",
      "Episode 49 reward: 3.5\n",
      "Episode 59 reward: 3.3\n",
      "Episode 69 reward: 2.8\n",
      "Episode 79 reward: 3.3\n",
      "Episode 89 reward: 3.6\n",
      "Episode 99 reward: 4.4\n",
      "Episode 109 reward: 3.1\n",
      "Episode 119 reward: 4.5\n",
      "Episode 129 reward: 3.2\n",
      "Episode 139 reward: 5.8\n",
      "Episode 149 reward: 4.6\n",
      "Episode 159 reward: 4.0\n",
      "Episode 169 reward: 5.4\n",
      "Episode 179 reward: 4.7\n",
      "Episode 189 reward: 5.3\n",
      "Episode 199 reward: 5.6\n",
      "Episode 209 reward: 6.0\n",
      "Episode 219 reward: 8.0\n",
      "Episode 229 reward: 6.1\n",
      "Episode 239 reward: 4.0\n",
      "Episode 249 reward: 7.4\n",
      "Episode 259 reward: 11.8\n",
      "Episode 269 reward: 9.7\n",
      "Episode 279 reward: 11.8\n",
      "Episode 289 reward: 13.6\n",
      "Episode 299 reward: 10.8\n",
      "Episode 309 reward: 13.6\n",
      "Episode 319 reward: 22.3\n",
      "Episode 329 reward: 17.2\n",
      "Episode 339 reward: 19.9\n",
      "Episode 349 reward: 21.6\n",
      "Episode 359 reward: 17.3\n",
      "Episode 369 reward: 18.3\n",
      "Episode 379 reward: 23.4\n",
      "Episode 389 reward: 22.5\n",
      "Episode 399 reward: 23.0\n",
      "Episode 409 reward: 30.2\n",
      "Episode 419 reward: 29.0\n",
      "Episode 429 reward: 28.3\n",
      "Episode 439 reward: 32.8\n",
      "Episode 449 reward: 35.5\n",
      "Episode 459 reward: 34.0\n",
      "Episode 469 reward: 37.9\n",
      "Episode 479 reward: 35.9\n",
      "Episode 489 reward: 37.3\n",
      "Episode 499 reward: 40.4\n",
      "Episode 509 reward: 38.1\n",
      "Episode 519 reward: 41.6\n",
      "Episode 529 reward: 38.6\n",
      "Episode 539 reward: 41.7\n",
      "Episode 549 reward: 39.8\n",
      "Episode 559 reward: 36.8\n",
      "Episode 569 reward: 38.2\n",
      "Episode 579 reward: 37.3\n",
      "Episode 589 reward: 37.2\n",
      "Episode 599 reward: 39.9\n",
      "Episode 609 reward: 40.4\n",
      "Episode 619 reward: 39.4\n",
      "Episode 629 reward: 37.0\n",
      "Episode 639 reward: 40.3\n",
      "Episode 649 reward: 42.3\n",
      "Episode 659 reward: 44.1\n",
      "Episode 669 reward: 37.5\n",
      "Episode 679 reward: 41.6\n",
      "Episode 689 reward: 40.6\n",
      "Episode 699 reward: 44.6\n",
      "Episode 709 reward: 39.7\n",
      "Episode 719 reward: 43.7\n",
      "Episode 729 reward: 40.6\n",
      "Episode 739 reward: 40.6\n",
      "Episode 749 reward: 34.3\n",
      "Episode 759 reward: 43.4\n",
      "Episode 769 reward: 39.5\n",
      "Episode 779 reward: 42.2\n",
      "Episode 789 reward: 42.5\n",
      "Episode 799 reward: 39.3\n",
      "Episode 809 reward: 43.1\n",
      "Episode 819 reward: 42.5\n",
      "Episode 829 reward: 42.7\n",
      "Episode 839 reward: 38.4\n",
      "Episode 849 reward: 38.8\n",
      "Episode 859 reward: 44.1\n",
      "Episode 869 reward: 37.6\n",
      "Episode 879 reward: 39.1\n",
      "Episode 889 reward: 38.0\n",
      "Episode 899 reward: 35.0\n",
      "Episode 909 reward: 40.6\n",
      "Episode 919 reward: 43.7\n",
      "Episode 929 reward: 45.3\n",
      "Episode 939 reward: 39.7\n",
      "Episode 949 reward: 43.7\n",
      "Episode 959 reward: 42.5\n",
      "Episode 969 reward: 40.7\n",
      "Episode 979 reward: 40.8\n",
      "Episode 989 reward: 38.8\n",
      "Episode 999 reward: 43.8\n",
      "Episode 1009 reward: 37.6\n",
      "Episode 1019 reward: 36.4\n",
      "Episode 1029 reward: 39.6\n",
      "Episode 1039 reward: 36.8\n",
      "Episode 1049 reward: 40.0\n",
      "Episode 1059 reward: 36.6\n",
      "Episode 1069 reward: 32.4\n",
      "Episode 1079 reward: 41.2\n",
      "Episode 1089 reward: 42.0\n",
      "Episode 1099 reward: 40.0\n",
      "Episode 1109 reward: 41.1\n",
      "Episode 1119 reward: 38.0\n",
      "Episode 1129 reward: 37.0\n",
      "Episode 1139 reward: 39.5\n",
      "Episode 1149 reward: 38.9\n",
      "Episode 1159 reward: 38.5\n",
      "Episode 1169 reward: 43.1\n",
      "Episode 1179 reward: 35.7\n",
      "Episode 1189 reward: 41.5\n",
      "Episode 1199 reward: 36.3\n",
      "Episode 1209 reward: 38.3\n",
      "Episode 1219 reward: 40.4\n",
      "Episode 1229 reward: 40.6\n",
      "Episode 1239 reward: 38.1\n",
      "Episode 1249 reward: 36.4\n",
      "Episode 1259 reward: 47.6\n",
      "Episode 1269 reward: 39.1\n",
      "Episode 1279 reward: 40.6\n",
      "Episode 1289 reward: 42.1\n",
      "Episode 1299 reward: 40.2\n",
      "Episode 1309 reward: 40.2\n",
      "Episode 1319 reward: 37.7\n",
      "Episode 1329 reward: 39.8\n",
      "Episode 1339 reward: 38.5\n",
      "Episode 1349 reward: 40.6\n",
      "Episode 1359 reward: 42.1\n",
      "Episode 1369 reward: 36.5\n",
      "Episode 1379 reward: 41.0\n",
      "Episode 1389 reward: 39.5\n",
      "Episode 1399 reward: 35.4\n",
      "Episode 1409 reward: 39.5\n",
      "Episode 1419 reward: 37.8\n",
      "Episode 1429 reward: 39.4\n",
      "Episode 1439 reward: 43.4\n",
      "Episode 1449 reward: 38.8\n",
      "Episode 1459 reward: 42.8\n",
      "Episode 1469 reward: 44.5\n",
      "Episode 1479 reward: 37.8\n",
      "Episode 1489 reward: 37.8\n",
      "Episode 1499 reward: 35.3\n",
      "Episode 1509 reward: 40.4\n",
      "Episode 1519 reward: 39.8\n",
      "Episode 1529 reward: 39.2\n",
      "Episode 1539 reward: 38.0\n",
      "Episode 1549 reward: 36.5\n",
      "Episode 1559 reward: 45.0\n",
      "Episode 1569 reward: 42.9\n",
      "Episode 1579 reward: 40.0\n",
      "Episode 1589 reward: 36.8\n",
      "Episode 1599 reward: 39.1\n",
      "Episode 1609 reward: 39.2\n",
      "Episode 1619 reward: 40.8\n",
      "Episode 1629 reward: 34.0\n",
      "Episode 1639 reward: 42.0\n",
      "Episode 1649 reward: 35.5\n",
      "Episode 1659 reward: 40.1\n",
      "Episode 1669 reward: 41.1\n",
      "Episode 1679 reward: 39.2\n",
      "Episode 1689 reward: 43.9\n",
      "Episode 1699 reward: 41.0\n",
      "Episode 1709 reward: 37.8\n",
      "Episode 1719 reward: 40.9\n",
      "Episode 1729 reward: 42.1\n",
      "Episode 1739 reward: 40.0\n",
      "Episode 1749 reward: 33.1\n",
      "Episode 1759 reward: 41.3\n",
      "Episode 1769 reward: 40.2\n",
      "Episode 1779 reward: 41.0\n",
      "Episode 1789 reward: 42.4\n",
      "Episode 1799 reward: 35.9\n",
      "Episode 1809 reward: 37.6\n",
      "Episode 1819 reward: 36.1\n",
      "Episode 1829 reward: 38.4\n",
      "Episode 1839 reward: 42.5\n",
      "Episode 1849 reward: 40.6\n",
      "Episode 1859 reward: 41.2\n",
      "Episode 1869 reward: 39.4\n",
      "Episode 1879 reward: 41.9\n",
      "Episode 1889 reward: 35.1\n",
      "Episode 1899 reward: 39.3\n",
      "Episode 1909 reward: 32.9\n",
      "Episode 1919 reward: 41.7\n",
      "Episode 1929 reward: 42.2\n",
      "Episode 1939 reward: 40.5\n",
      "Episode 1949 reward: 38.1\n",
      "Episode 1959 reward: 46.3\n",
      "Episode 1969 reward: 38.1\n",
      "Episode 1979 reward: 45.6\n",
      "Episode 1989 reward: 39.9\n",
      "Episode 1999 reward: 36.0\n",
      "Episode 2009 reward: 38.8\n",
      "Episode 2019 reward: 45.0\n",
      "Episode 2029 reward: 40.2\n",
      "Episode 2039 reward: 37.3\n",
      "Episode 2049 reward: 38.7\n",
      "Episode 2059 reward: 40.6\n",
      "Episode 2069 reward: 40.2\n",
      "Episode 2079 reward: 38.5\n",
      "Episode 2089 reward: 38.1\n",
      "Episode 2099 reward: 38.6\n",
      "Episode 2109 reward: 38.5\n",
      "Episode 2119 reward: 39.3\n",
      "Episode 2129 reward: 37.7\n",
      "Episode 2139 reward: 43.6\n",
      "Episode 2149 reward: 38.2\n",
      "Episode 2159 reward: 39.5\n",
      "Episode 2169 reward: 41.1\n",
      "Episode 2179 reward: 40.6\n",
      "Episode 2189 reward: 37.8\n",
      "Episode 2199 reward: 44.4\n",
      "Episode 2209 reward: 42.8\n",
      "Episode 2219 reward: 39.3\n",
      "Episode 2229 reward: 40.6\n",
      "Episode 2239 reward: 41.7\n",
      "Episode 2249 reward: 38.7\n",
      "Episode 2259 reward: 39.4\n",
      "Episode 2269 reward: 43.6\n",
      "Episode 2279 reward: 38.7\n",
      "Episode 2289 reward: 39.7\n",
      "Episode 2299 reward: 44.3\n",
      "Episode 2309 reward: 36.6\n",
      "Episode 2319 reward: 40.6\n",
      "Episode 2329 reward: 36.1\n",
      "Episode 2339 reward: 38.4\n",
      "Episode 2349 reward: 40.8\n",
      "Episode 2359 reward: 36.3\n",
      "Episode 2369 reward: 39.4\n",
      "Episode 2379 reward: 40.2\n",
      "Episode 2389 reward: 40.2\n",
      "Episode 2399 reward: 44.5\n",
      "Episode 2409 reward: 36.0\n",
      "Episode 2419 reward: 44.2\n",
      "Episode 2429 reward: 37.7\n",
      "Episode 2439 reward: 38.6\n",
      "Episode 2449 reward: 39.9\n",
      "Episode 2459 reward: 40.2\n",
      "Episode 2469 reward: 40.2\n",
      "Episode 2479 reward: 42.0\n",
      "Episode 2489 reward: 45.2\n",
      "Episode 2499 reward: 45.2\n",
      "Episode 2509 reward: 42.0\n",
      "Episode 2519 reward: 38.8\n",
      "Episode 2529 reward: 40.0\n",
      "Episode 2539 reward: 35.0\n",
      "Episode 2549 reward: 40.9\n",
      "Episode 2559 reward: 43.2\n",
      "Episode 2569 reward: 39.4\n",
      "Episode 2579 reward: 43.8\n",
      "Episode 2589 reward: 36.4\n",
      "Episode 2599 reward: 44.3\n",
      "Episode 2609 reward: 39.0\n",
      "Episode 2619 reward: 33.6\n",
      "Episode 2629 reward: 43.9\n",
      "Episode 2639 reward: 41.3\n",
      "Episode 2649 reward: 41.9\n",
      "Episode 2659 reward: 38.1\n",
      "Episode 2669 reward: 41.6\n",
      "Episode 2679 reward: 41.6\n",
      "Episode 2689 reward: 39.2\n",
      "Episode 2699 reward: 38.5\n",
      "Episode 2709 reward: 37.1\n",
      "Episode 2719 reward: 39.1\n",
      "Episode 2729 reward: 41.0\n",
      "Episode 2739 reward: 41.7\n",
      "Episode 2749 reward: 36.9\n",
      "Episode 2759 reward: 41.7\n",
      "Episode 2769 reward: 36.1\n",
      "Episode 2779 reward: 40.9\n",
      "Episode 2789 reward: 41.6\n",
      "Episode 2799 reward: 40.9\n",
      "Episode 2809 reward: 43.1\n",
      "Episode 2819 reward: 39.1\n",
      "Episode 2829 reward: 42.4\n",
      "Episode 2839 reward: 43.1\n",
      "Episode 2849 reward: 43.1\n",
      "Episode 2859 reward: 41.8\n",
      "Episode 2869 reward: 38.7\n",
      "Episode 2879 reward: 47.3\n",
      "Episode 2889 reward: 39.0\n",
      "Episode 2899 reward: 39.3\n",
      "Episode 2909 reward: 42.2\n",
      "Episode 2919 reward: 36.8\n",
      "Episode 2929 reward: 40.4\n",
      "Episode 2939 reward: 37.2\n",
      "Episode 2949 reward: 42.5\n",
      "Episode 2959 reward: 42.7\n",
      "Episode 2969 reward: 33.7\n",
      "Episode 2979 reward: 37.5\n",
      "Episode 2989 reward: 43.9\n",
      "Episode 2999 reward: 45.1\n",
      "Episode 3009 reward: 39.9\n",
      "Episode 3019 reward: 40.2\n",
      "Episode 3029 reward: 44.7\n",
      "Episode 3039 reward: 35.0\n",
      "Episode 3049 reward: 42.7\n",
      "Episode 3059 reward: 45.1\n",
      "Episode 3069 reward: 39.6\n",
      "Episode 3079 reward: 41.8\n",
      "Episode 3089 reward: 44.2\n",
      "Episode 3099 reward: 38.3\n",
      "Episode 3109 reward: 35.1\n",
      "Episode 3119 reward: 38.6\n",
      "Episode 3129 reward: 41.8\n",
      "Episode 3139 reward: 38.3\n",
      "Episode 3149 reward: 44.3\n",
      "Episode 3159 reward: 37.0\n",
      "Episode 3169 reward: 41.4\n",
      "Episode 3179 reward: 38.4\n",
      "Episode 3189 reward: 46.2\n",
      "Episode 3199 reward: 37.3\n",
      "Episode 3209 reward: 43.3\n",
      "Episode 3219 reward: 37.7\n",
      "Episode 3229 reward: 36.6\n",
      "Episode 3239 reward: 42.6\n",
      "Episode 3249 reward: 37.0\n",
      "Episode 3259 reward: 39.7\n",
      "Episode 3269 reward: 42.0\n",
      "Episode 3279 reward: 41.4\n",
      "Episode 3289 reward: 39.3\n",
      "Episode 3299 reward: 38.7\n",
      "Episode 3309 reward: 38.5\n",
      "Episode 3319 reward: 34.0\n",
      "Episode 3329 reward: 40.5\n",
      "Episode 3339 reward: 37.9\n",
      "Episode 3349 reward: 36.4\n",
      "Episode 3359 reward: 40.5\n",
      "Episode 3369 reward: 44.4\n",
      "Episode 3379 reward: 44.8\n",
      "Episode 3389 reward: 41.4\n",
      "Episode 3399 reward: 43.3\n",
      "Episode 3409 reward: 32.4\n",
      "Episode 3419 reward: 33.8\n",
      "Episode 3429 reward: 42.9\n",
      "Episode 3439 reward: 37.0\n",
      "Episode 3449 reward: 41.7\n",
      "Episode 3459 reward: 39.1\n",
      "Episode 3469 reward: 40.6\n",
      "Episode 3479 reward: 40.2\n",
      "Episode 3489 reward: 39.6\n",
      "Episode 3499 reward: 39.7\n",
      "Episode 3509 reward: 37.3\n",
      "Episode 3519 reward: 43.5\n",
      "Episode 3529 reward: 41.3\n",
      "Episode 3539 reward: 39.5\n",
      "Episode 3549 reward: 40.1\n",
      "Episode 3559 reward: 46.1\n",
      "Episode 3569 reward: 39.2\n",
      "Episode 3579 reward: 40.6\n",
      "Episode 3589 reward: 44.3\n",
      "Episode 3599 reward: 42.7\n",
      "Episode 3609 reward: 44.9\n",
      "Episode 3619 reward: 39.9\n",
      "Episode 3629 reward: 37.1\n",
      "Episode 3639 reward: 43.7\n",
      "Episode 3649 reward: 38.3\n",
      "Episode 3659 reward: 43.3\n",
      "Episode 3669 reward: 43.3\n",
      "Episode 3679 reward: 39.2\n",
      "Episode 3689 reward: 39.3\n",
      "Episode 3699 reward: 41.4\n",
      "Episode 3709 reward: 41.7\n",
      "Episode 3719 reward: 37.7\n",
      "Episode 3729 reward: 38.0\n",
      "Episode 3739 reward: 42.2\n",
      "Episode 3749 reward: 42.0\n",
      "Episode 3759 reward: 39.0\n",
      "Episode 3769 reward: 43.8\n",
      "Episode 3779 reward: 41.0\n",
      "Episode 3789 reward: 40.2\n",
      "Episode 3799 reward: 39.1\n",
      "Episode 3809 reward: 39.4\n",
      "Episode 3819 reward: 43.3\n",
      "Episode 3829 reward: 45.2\n",
      "Episode 3839 reward: 42.2\n",
      "Episode 3849 reward: 43.0\n",
      "Episode 3859 reward: 38.5\n",
      "Episode 3869 reward: 39.5\n",
      "Episode 3879 reward: 38.9\n",
      "Episode 3889 reward: 41.7\n",
      "Episode 3899 reward: 39.7\n",
      "Episode 3909 reward: 36.9\n",
      "Episode 3919 reward: 42.3\n",
      "Episode 3929 reward: 41.4\n",
      "Episode 3939 reward: 43.6\n",
      "Episode 3949 reward: 39.0\n",
      "Episode 3959 reward: 39.6\n",
      "Episode 3969 reward: 39.4\n",
      "Episode 3979 reward: 39.5\n",
      "Episode 3989 reward: 38.4\n",
      "Episode 3999 reward: 39.9\n",
      "Episode 4009 reward: 40.5\n",
      "Episode 4019 reward: 41.9\n",
      "Episode 4029 reward: 34.3\n",
      "Episode 4039 reward: 38.9\n",
      "Episode 4049 reward: 44.3\n",
      "Episode 4059 reward: 41.6\n",
      "Episode 4069 reward: 41.1\n",
      "Episode 4079 reward: 44.6\n",
      "Episode 4089 reward: 42.9\n",
      "Episode 4099 reward: 43.5\n",
      "Episode 4109 reward: 35.4\n",
      "Episode 4119 reward: 41.6\n",
      "Episode 4129 reward: 34.1\n",
      "Episode 4139 reward: 39.5\n",
      "Episode 4149 reward: 41.0\n",
      "Episode 4159 reward: 39.2\n",
      "Episode 4169 reward: 42.7\n",
      "Episode 4179 reward: 40.3\n",
      "Episode 4189 reward: 45.1\n",
      "Episode 4199 reward: 41.7\n",
      "Episode 4209 reward: 40.2\n",
      "Episode 4219 reward: 36.9\n",
      "Episode 4229 reward: 36.5\n",
      "Episode 4239 reward: 40.3\n",
      "Episode 4249 reward: 34.6\n",
      "Episode 4259 reward: 41.6\n",
      "Episode 4269 reward: 37.8\n",
      "Episode 4279 reward: 41.1\n",
      "Episode 4289 reward: 36.5\n",
      "Episode 4299 reward: 40.3\n",
      "Episode 4309 reward: 35.2\n",
      "Episode 4319 reward: 31.9\n",
      "Episode 4329 reward: 40.9\n",
      "Episode 4339 reward: 39.9\n",
      "Episode 4349 reward: 38.6\n",
      "Episode 4359 reward: 42.9\n",
      "Episode 4369 reward: 41.5\n",
      "Episode 4379 reward: 41.5\n",
      "Episode 4389 reward: 37.0\n",
      "Episode 4399 reward: 43.9\n",
      "Episode 4409 reward: 41.4\n",
      "Episode 4419 reward: 36.8\n",
      "Episode 4429 reward: 31.2\n",
      "Episode 4439 reward: 36.0\n",
      "Episode 4449 reward: 42.3\n",
      "Episode 4459 reward: 40.9\n",
      "Episode 4469 reward: 46.2\n",
      "Episode 4479 reward: 39.4\n",
      "Episode 4489 reward: 40.8\n",
      "Episode 4499 reward: 40.1\n",
      "Episode 4509 reward: 41.7\n",
      "Episode 4519 reward: 37.8\n",
      "Episode 4529 reward: 44.2\n",
      "Episode 4539 reward: 41.2\n",
      "Episode 4549 reward: 38.8\n",
      "Episode 4559 reward: 43.3\n",
      "Episode 4569 reward: 42.4\n",
      "Episode 4579 reward: 40.1\n",
      "Episode 4589 reward: 40.1\n",
      "Episode 4599 reward: 37.7\n",
      "Episode 4609 reward: 42.0\n",
      "Episode 4619 reward: 36.4\n",
      "Episode 4629 reward: 33.4\n",
      "Episode 4639 reward: 41.7\n",
      "Episode 4649 reward: 44.0\n",
      "Episode 4659 reward: 38.6\n",
      "Episode 4669 reward: 42.7\n",
      "Episode 4679 reward: 38.5\n",
      "Episode 4689 reward: 42.4\n",
      "Episode 4699 reward: 40.8\n",
      "Episode 4709 reward: 44.7\n",
      "Episode 4719 reward: 38.0\n",
      "Episode 4729 reward: 44.6\n",
      "Episode 4739 reward: 40.6\n",
      "Episode 4749 reward: 42.8\n",
      "Episode 4759 reward: 37.2\n",
      "Episode 4769 reward: 44.7\n",
      "Episode 4779 reward: 34.9\n",
      "Episode 4789 reward: 39.5\n",
      "Episode 4799 reward: 45.0\n",
      "Episode 4809 reward: 38.6\n",
      "Episode 4819 reward: 43.6\n",
      "Episode 4829 reward: 39.2\n",
      "Episode 4839 reward: 40.2\n",
      "Episode 4849 reward: 40.8\n",
      "Episode 4859 reward: 37.7\n",
      "Episode 4869 reward: 44.4\n",
      "Episode 4879 reward: 29.4\n",
      "Episode 4889 reward: 39.6\n",
      "Episode 4899 reward: 38.8\n",
      "Episode 4909 reward: 46.2\n",
      "Episode 4919 reward: 45.2\n",
      "Episode 4929 reward: 42.0\n",
      "Episode 4939 reward: 38.4\n",
      "Episode 4949 reward: 42.5\n",
      "Episode 4959 reward: 36.0\n",
      "Episode 4969 reward: 44.0\n",
      "Episode 4979 reward: 40.4\n",
      "Episode 4989 reward: 39.2\n",
      "Episode 4999 reward: 40.0\n",
      "Mean reward per episode: 37.5138\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 50 moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            total_steps += 1\n",
    "            \n",
    "            ################################################################################\n",
    "            # TODO: Save the experience to our episode buffer.                             #\n",
    "            # You will need to do the following:                                           #\n",
    "            # (1) Get new state s1 (resized), reward r and done d from a                   #\n",
    "            # (2) Add experience to episode buffer. Hint: experience includes              #\n",
    "            #     s, a, r, s1 and d.                                                       #\n",
    "            ################################################################################\n",
    "            s1, r, d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            experience = np.expand_dims(np.array([s, a, r, s1, d]), 0)\n",
    "            episodeBuffer.add(experience)   \n",
    "            ################################################################################\n",
    "            #                                 END OF YOUR CODE                             #\n",
    "            ################################################################################\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO: Implement Double-DQN                                                   #\n",
    "                    # (1) Get a random batch of experiences via experience_buffer class            #\n",
    "                    #                                                                              #\n",
    "                    # (2) Perform the Double-DQN update to the target Q-values                     #\n",
    "                    #     Hint: Use mainQN and targetQN separately to chose an action and predict  #\n",
    "                    #     the Q-values for that action.                                            #\n",
    "                    #     Then compute targetQ based on Double-DQN equation                        #\n",
    "                    #                                                                              #\n",
    "                    # (3) Update the primary network with our target values                        #\n",
    "                    ################################################################################ \n",
    "                    batch = myBuffer.sample(batch_size)\n",
    "                    \n",
    "                    stacked_state = np.vstack(batch[:, 3])\n",
    "                    \n",
    "                    action_ = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput: stacked_state})\n",
    "                    Q_ = sess.run(targetQN.Qout, feed_dict={targetQN.scalarInput: stacked_state})\n",
    "                    next_Q = Q_[range(batch_size), action_]\n",
    "                    \n",
    "                    done_mask = 1 - batch[:, 4]\n",
    "                    targetQ = batch[:, 2] + done_mask * y * next_Q\n",
    "                    \n",
    "                    sess.run(mainQN.updateModel, feed_dict={mainQN.scalarInput: np.vstack(batch[:,0]), \n",
    "                                                             mainQN.targetQ: targetQ, \n",
    "                                                             mainQN.actions: batch[:,1]})\n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "                           \n",
    "                    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(\"Episode\",i,\"reward:\",np.mean(rList[-10:]))\n",
    "print(\"Mean reward per episode: \" + str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally the mean reward is ~19, now it is ~ 37.5, almost a x2 improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking network learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14b1747c24a8>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_2018u2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
