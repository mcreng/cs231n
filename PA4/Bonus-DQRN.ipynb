{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Navigation with DQRN\n",
    "\n",
    "This is using most of the codes from DQN to build a Deep Q Recurrent Network. There are three differences to DQN:\n",
    "- After the last CONV layer, a LSTM is added to capture temporal information.\n",
    "- We instead of sample the whole experience, we sample part of an experience with a fixed length.\n",
    "- In passing gradient, we mask away the first half of the gradient since they are not as useful.\n",
    "\n",
    "Reference: [Medium post](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the game environment\n",
    "Notice we have `partial=True`, which means the agent can only see a local region around it isntead of the whole field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng/4boXn5b/LLzRUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x147695d6a390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "env = gameEnv(partial=True,size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Buffer\n",
    "We sample parts of experiences with fixed length now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 1000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self,batch_size,trace_length):\n",
    "        sampled_episodes = random.sample(self.buffer,batch_size)\n",
    "        sampledTraces = []\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0,len(episode)+1-trace_length)\n",
    "            sampledTraces.append(episode[point:point+trace_length])\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        return np.reshape(sampledTraces,[batch_size*trace_length,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple function to resize our game frames:\n",
    "\n",
    "Modification: added `/255.0` otherwise network might explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168]) / 255.0 # 84 x 84 x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions allow us to update the parameters of our target network with those of the primary network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself\n",
    "Modification: Added LSTM, masked gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size,lstm_cell,scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            #The network recieves a frame from the game, flattened into an array.\n",
    "            #It then resizes it and processes it through four convolutional layers.\n",
    "            #We use slim.conv2d to set up our network \n",
    "            self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "            self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "            self.conv1 = slim.conv2d( \\\n",
    "                inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "            self.conv2 = slim.conv2d( \\\n",
    "                inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "            self.conv3 = slim.conv2d( \\\n",
    "                inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "            self.conv4 = slim.conv2d( \\\n",
    "                inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "\n",
    "            \n",
    "            self.lstm_input_size = tf.placeholder(shape=[], dtype=tf.int32)\n",
    "            self.batch_size = tf.placeholder(shape=[], dtype=tf.int32)\n",
    "            self.lstm_state_in = lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "            \n",
    "            flat = tf.layers.flatten(self.conv4)\n",
    "            flat = tf.reshape(flat, (self.batch_size, self.lstm_input_size, h_size))\n",
    "            self.lstm, self.lstm_state = tf.nn.dynamic_rnn(inputs=flat,\n",
    "                                                            cell=lstm_cell,\n",
    "                                                            dtype=tf.float32,\n",
    "                                                            initial_state=self.lstm_state_in)\n",
    "            self.lstm = tf.reshape(self.lstm, shape=[-1, h_size])\n",
    "\n",
    "            ################################################################################\n",
    "            # TODO: Implement Dueling DQN                                                  #\n",
    "            # We take the output from the final convolutional layer i.e. self.conv4 and    #\n",
    "            # split it into separate advantage and value streams.                          #\n",
    "            # Outout: self.Advantage, self.Value                                           #\n",
    "            # Hint: Refer to Fig.1 in [Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)  #\n",
    "            #       In implementation, use tf.split to split into two branches. You may    #\n",
    "            #       use xavier_initializer for initializing the two additional linear      #\n",
    "            #       layers.                                                                # \n",
    "            ################################################################################\n",
    "            adv, val = tf.split(self.lstm, 2, 1)\n",
    "            self.Advantage = tf.layers.dense(slim.flatten(adv), env.actions)\n",
    "            self.Value = tf.layers.dense(slim.flatten(val), 1)\n",
    "            ################################################################################\n",
    "            #                                 END OF YOUR CODE                             #\n",
    "            ################################################################################\n",
    "\n",
    "            #Then combine them together to get our final Q-values. \n",
    "            #Please refer to Equation (9) in [Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)\n",
    "            self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "            self.predict = tf.argmax(self.Qout,1)\n",
    "\n",
    "            #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "            self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "\n",
    "            ################################################################################\n",
    "            # TODO:                                                                        #\n",
    "            # Obtain the loss (self.loss) by taking the sum of squares difference          #\n",
    "            # between the target and prediction Q values.                                  #\n",
    "            ################################################################################\n",
    "            predictQ = tf.reduce_sum(self.Qout*self.actions_onehot,axis = 1)\n",
    "\n",
    "            # Lample & Chatlot 2016\n",
    "            mask = tf.concat([tf.zeros([self.batch_size, self.lstm_input_size//2]),\n",
    "                                   tf.ones([self.batch_size, self.lstm_input_size//2])],1)\n",
    "            mask = tf.reshape(mask, [-1])\n",
    "\n",
    "            self.loss = tf.reduce_mean((predictQ - self.targetQ)**2 * mask)\n",
    "            ################################################################################\n",
    "            #                                 END OF YOUR CODE                             #\n",
    "            ################################################################################\n",
    "\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "            self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting all the training parameters\n",
    "\n",
    "Modification:\n",
    "- Added trace_length\n",
    "- In each epsiode, LSTM states are recycled.\n",
    "- After each episode, LSTM states are reset to 0 and the data of the whole episode is used to do back prop.\n",
    "- States are reset to 0 again after back prop for new episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 #How many experience traces to use for each training step.\n",
    "trace_length = 8 #How long each experience trace will be when training\n",
    "update_freq = 5 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-ef6547f267e3>:46: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Episode 9 reward: 3.1\n",
      "Episode 19 reward: 2.6\n",
      "Episode 29 reward: 3.2\n",
      "Episode 39 reward: 2.1\n",
      "Episode 49 reward: 1.5\n",
      "Episode 59 reward: 1.8\n",
      "Episode 69 reward: 2.9\n",
      "Episode 79 reward: 2.1\n",
      "Episode 89 reward: 1.8\n",
      "Episode 99 reward: 1.8\n",
      "Episode 109 reward: 3.5\n",
      "Episode 119 reward: 1.2\n",
      "Episode 129 reward: 0.6\n",
      "Episode 139 reward: 2.5\n",
      "Episode 149 reward: 2.6\n",
      "Episode 159 reward: -0.3\n",
      "Episode 169 reward: 2.7\n",
      "Episode 179 reward: 1.4\n",
      "Episode 189 reward: 1.4\n",
      "Episode 199 reward: 1.5\n",
      "Episode 209 reward: 2.4\n",
      "Episode 219 reward: 2.6\n",
      "Episode 229 reward: 4.6\n",
      "Episode 239 reward: 2.9\n",
      "Episode 249 reward: 5.5\n",
      "Episode 259 reward: 4.4\n",
      "Episode 269 reward: 2.7\n",
      "Episode 279 reward: 4.9\n",
      "Episode 289 reward: 6.5\n",
      "Episode 299 reward: 5.5\n",
      "Episode 309 reward: 5.0\n",
      "Episode 319 reward: 5.0\n",
      "Episode 329 reward: 8.7\n",
      "Episode 339 reward: 5.5\n",
      "Episode 349 reward: 6.2\n",
      "Episode 359 reward: 6.8\n",
      "Episode 369 reward: 6.3\n",
      "Episode 379 reward: 7.5\n",
      "Episode 389 reward: 7.0\n",
      "Episode 399 reward: 11.4\n",
      "Episode 409 reward: 8.7\n",
      "Episode 419 reward: 10.3\n",
      "Episode 429 reward: 13.3\n",
      "Episode 439 reward: 6.8\n",
      "Episode 449 reward: 10.3\n",
      "Episode 459 reward: 9.7\n",
      "Episode 469 reward: 13.4\n",
      "Episode 479 reward: 11.9\n",
      "Episode 489 reward: 13.2\n",
      "Episode 499 reward: 12.6\n",
      "Episode 509 reward: 13.3\n",
      "Episode 519 reward: 11.6\n",
      "Episode 529 reward: 14.2\n",
      "Episode 539 reward: 15.8\n",
      "Episode 549 reward: 15.0\n",
      "Episode 559 reward: 13.3\n",
      "Episode 569 reward: 15.1\n",
      "Episode 579 reward: 14.4\n",
      "Episode 589 reward: 15.8\n",
      "Episode 599 reward: 18.0\n",
      "Episode 609 reward: 16.8\n",
      "Episode 619 reward: 15.4\n",
      "Episode 629 reward: 13.5\n",
      "Episode 639 reward: 17.4\n",
      "Episode 649 reward: 16.9\n",
      "Episode 659 reward: 15.8\n",
      "Episode 669 reward: 14.6\n",
      "Episode 679 reward: 16.0\n",
      "Episode 689 reward: 18.3\n",
      "Episode 699 reward: 16.6\n",
      "Episode 709 reward: 19.5\n",
      "Episode 719 reward: 17.2\n",
      "Episode 729 reward: 18.8\n",
      "Episode 739 reward: 18.2\n",
      "Episode 749 reward: 16.9\n",
      "Episode 759 reward: 18.7\n",
      "Episode 769 reward: 15.3\n",
      "Episode 779 reward: 16.2\n",
      "Episode 789 reward: 17.7\n",
      "Episode 799 reward: 17.1\n",
      "Episode 809 reward: 18.0\n",
      "Episode 819 reward: 16.4\n",
      "Episode 829 reward: 20.9\n",
      "Episode 839 reward: 17.7\n",
      "Episode 849 reward: 16.1\n",
      "Episode 859 reward: 16.6\n",
      "Episode 869 reward: 19.3\n",
      "Episode 879 reward: 18.8\n",
      "Episode 889 reward: 18.5\n",
      "Episode 899 reward: 18.8\n",
      "Episode 909 reward: 19.1\n",
      "Episode 919 reward: 19.1\n",
      "Episode 929 reward: 17.8\n",
      "Episode 939 reward: 17.1\n",
      "Episode 949 reward: 18.8\n",
      "Episode 959 reward: 19.2\n",
      "Episode 969 reward: 18.1\n",
      "Episode 979 reward: 21.3\n",
      "Episode 989 reward: 15.9\n",
      "Episode 999 reward: 17.2\n",
      "Episode 1009 reward: 20.3\n",
      "Episode 1019 reward: 20.4\n",
      "Episode 1029 reward: 18.2\n",
      "Episode 1039 reward: 18.4\n",
      "Episode 1049 reward: 19.5\n",
      "Episode 1059 reward: 19.7\n",
      "Episode 1069 reward: 19.6\n",
      "Episode 1079 reward: 18.2\n",
      "Episode 1089 reward: 21.6\n",
      "Episode 1099 reward: 20.8\n",
      "Episode 1109 reward: 17.9\n",
      "Episode 1119 reward: 18.8\n",
      "Episode 1129 reward: 21.3\n",
      "Episode 1139 reward: 20.3\n",
      "Episode 1149 reward: 17.1\n",
      "Episode 1159 reward: 20.3\n",
      "Episode 1169 reward: 19.9\n",
      "Episode 1179 reward: 18.8\n",
      "Episode 1189 reward: 19.4\n",
      "Episode 1199 reward: 19.3\n",
      "Episode 1209 reward: 20.4\n",
      "Episode 1219 reward: 19.3\n",
      "Episode 1229 reward: 20.5\n",
      "Episode 1239 reward: 19.9\n",
      "Episode 1249 reward: 18.3\n",
      "Episode 1259 reward: 20.8\n",
      "Episode 1269 reward: 20.4\n",
      "Episode 1279 reward: 19.8\n",
      "Episode 1289 reward: 18.8\n",
      "Episode 1299 reward: 20.2\n",
      "Episode 1309 reward: 18.4\n",
      "Episode 1319 reward: 18.8\n",
      "Episode 1329 reward: 19.5\n",
      "Episode 1339 reward: 18.4\n",
      "Episode 1349 reward: 19.1\n",
      "Episode 1359 reward: 18.8\n",
      "Episode 1369 reward: 20.4\n",
      "Episode 1379 reward: 21.7\n",
      "Episode 1389 reward: 20.5\n",
      "Episode 1399 reward: 20.2\n",
      "Episode 1409 reward: 21.1\n",
      "Episode 1419 reward: 22.5\n",
      "Episode 1429 reward: 20.3\n",
      "Episode 1439 reward: 19.2\n",
      "Episode 1449 reward: 21.0\n",
      "Episode 1459 reward: 21.7\n",
      "Episode 1469 reward: 18.3\n",
      "Episode 1479 reward: 19.3\n",
      "Episode 1489 reward: 20.6\n",
      "Episode 1499 reward: 20.5\n",
      "Episode 1509 reward: 19.0\n",
      "Episode 1519 reward: 20.3\n",
      "Episode 1529 reward: 22.9\n",
      "Episode 1539 reward: 20.1\n",
      "Episode 1549 reward: 18.3\n",
      "Episode 1559 reward: 20.6\n",
      "Episode 1569 reward: 21.6\n",
      "Episode 1579 reward: 21.0\n",
      "Episode 1589 reward: 19.1\n",
      "Episode 1599 reward: 19.7\n",
      "Episode 1609 reward: 21.5\n",
      "Episode 1619 reward: 19.9\n",
      "Episode 1629 reward: 20.3\n",
      "Episode 1639 reward: 20.4\n",
      "Episode 1649 reward: 19.8\n",
      "Episode 1659 reward: 21.5\n",
      "Episode 1669 reward: 19.3\n",
      "Episode 1679 reward: 20.9\n",
      "Episode 1689 reward: 21.3\n",
      "Episode 1699 reward: 18.8\n",
      "Episode 1709 reward: 19.0\n",
      "Episode 1719 reward: 19.4\n",
      "Episode 1729 reward: 19.0\n",
      "Episode 1739 reward: 21.0\n",
      "Episode 1749 reward: 19.6\n",
      "Episode 1759 reward: 22.0\n",
      "Episode 1769 reward: 19.3\n",
      "Episode 1779 reward: 21.5\n",
      "Episode 1789 reward: 19.3\n",
      "Episode 1799 reward: 20.8\n",
      "Episode 1809 reward: 19.3\n",
      "Episode 1819 reward: 19.3\n",
      "Episode 1829 reward: 23.3\n",
      "Episode 1839 reward: 19.8\n",
      "Episode 1849 reward: 21.9\n",
      "Episode 1859 reward: 20.9\n",
      "Episode 1869 reward: 19.2\n",
      "Episode 1879 reward: 19.9\n",
      "Episode 1889 reward: 17.9\n",
      "Episode 1899 reward: 19.9\n",
      "Episode 1909 reward: 20.2\n",
      "Episode 1919 reward: 19.7\n",
      "Episode 1929 reward: 20.7\n",
      "Episode 1939 reward: 21.1\n",
      "Episode 1949 reward: 20.5\n",
      "Episode 1959 reward: 18.7\n",
      "Episode 1969 reward: 20.3\n",
      "Episode 1979 reward: 19.2\n",
      "Episode 1989 reward: 20.9\n",
      "Episode 1999 reward: 20.3\n",
      "Episode 2009 reward: 19.1\n",
      "Episode 2019 reward: 20.4\n",
      "Episode 2029 reward: 21.4\n",
      "Episode 2039 reward: 19.4\n",
      "Episode 2049 reward: 20.5\n",
      "Episode 2059 reward: 20.1\n",
      "Episode 2069 reward: 21.7\n",
      "Episode 2079 reward: 18.5\n",
      "Episode 2089 reward: 19.8\n",
      "Episode 2099 reward: 22.0\n",
      "Episode 2109 reward: 19.8\n",
      "Episode 2119 reward: 18.4\n",
      "Episode 2129 reward: 19.6\n",
      "Episode 2139 reward: 19.9\n",
      "Episode 2149 reward: 19.7\n",
      "Episode 2159 reward: 20.4\n",
      "Episode 2169 reward: 22.4\n",
      "Episode 2179 reward: 20.7\n",
      "Episode 2189 reward: 20.0\n",
      "Episode 2199 reward: 20.9\n",
      "Episode 2209 reward: 22.2\n",
      "Episode 2219 reward: 21.4\n",
      "Episode 2229 reward: 18.7\n",
      "Episode 2239 reward: 22.2\n",
      "Episode 2249 reward: 20.7\n",
      "Episode 2259 reward: 21.9\n",
      "Episode 2269 reward: 21.7\n",
      "Episode 2279 reward: 20.1\n",
      "Episode 2289 reward: 21.4\n",
      "Episode 2299 reward: 21.0\n",
      "Episode 2309 reward: 21.1\n",
      "Episode 2319 reward: 18.8\n",
      "Episode 2329 reward: 19.2\n",
      "Episode 2339 reward: 20.2\n",
      "Episode 2349 reward: 20.0\n",
      "Episode 2359 reward: 19.8\n",
      "Episode 2369 reward: 23.0\n",
      "Episode 2379 reward: 19.3\n",
      "Episode 2389 reward: 20.5\n",
      "Episode 2399 reward: 21.8\n",
      "Episode 2409 reward: 21.3\n",
      "Episode 2419 reward: 20.0\n",
      "Episode 2429 reward: 22.6\n",
      "Episode 2439 reward: 22.2\n",
      "Episode 2449 reward: 18.2\n",
      "Episode 2459 reward: 21.8\n",
      "Episode 2469 reward: 20.7\n",
      "Episode 2479 reward: 23.0\n",
      "Episode 2489 reward: 20.2\n",
      "Episode 2499 reward: 20.6\n",
      "Episode 2509 reward: 21.7\n",
      "Episode 2519 reward: 20.0\n",
      "Episode 2529 reward: 19.4\n",
      "Episode 2539 reward: 19.8\n",
      "Episode 2549 reward: 21.4\n",
      "Episode 2559 reward: 22.7\n",
      "Episode 2569 reward: 21.5\n",
      "Episode 2579 reward: 20.1\n",
      "Episode 2589 reward: 21.9\n",
      "Episode 2599 reward: 21.6\n",
      "Episode 2609 reward: 19.6\n",
      "Episode 2619 reward: 20.6\n",
      "Episode 2629 reward: 18.6\n",
      "Episode 2639 reward: 19.5\n",
      "Episode 2649 reward: 21.2\n",
      "Episode 2659 reward: 22.3\n",
      "Episode 2669 reward: 18.9\n",
      "Episode 2679 reward: 21.7\n",
      "Episode 2689 reward: 20.3\n",
      "Episode 2699 reward: 21.3\n",
      "Episode 2709 reward: 19.8\n",
      "Episode 2719 reward: 23.1\n",
      "Episode 2729 reward: 19.9\n",
      "Episode 2739 reward: 19.4\n",
      "Episode 2749 reward: 20.9\n",
      "Episode 2759 reward: 20.7\n",
      "Episode 2769 reward: 21.6\n",
      "Episode 2779 reward: 20.6\n",
      "Episode 2789 reward: 20.2\n",
      "Episode 2799 reward: 19.2\n",
      "Episode 2809 reward: 20.4\n",
      "Episode 2819 reward: 21.5\n",
      "Episode 2829 reward: 20.2\n",
      "Episode 2839 reward: 21.3\n",
      "Episode 2849 reward: 21.3\n",
      "Episode 2859 reward: 21.4\n",
      "Episode 2869 reward: 20.5\n",
      "Episode 2879 reward: 21.7\n",
      "Episode 2889 reward: 20.8\n",
      "Episode 2899 reward: 20.5\n",
      "Episode 2909 reward: 18.5\n",
      "Episode 2919 reward: 20.9\n",
      "Episode 2929 reward: 21.1\n",
      "Episode 2939 reward: 22.1\n",
      "Episode 2949 reward: 21.9\n",
      "Episode 2959 reward: 20.9\n",
      "Episode 2969 reward: 20.9\n",
      "Episode 2979 reward: 20.1\n",
      "Episode 2989 reward: 20.2\n",
      "Episode 2999 reward: 21.3\n",
      "Episode 3009 reward: 20.3\n",
      "Episode 3019 reward: 21.3\n",
      "Episode 3029 reward: 20.5\n",
      "Episode 3039 reward: 21.5\n",
      "Episode 3049 reward: 19.1\n",
      "Episode 3059 reward: 19.8\n",
      "Episode 3069 reward: 20.1\n",
      "Episode 3079 reward: 18.9\n",
      "Episode 3089 reward: 20.4\n",
      "Episode 3099 reward: 21.5\n",
      "Episode 3109 reward: 21.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3119 reward: 21.4\n",
      "Episode 3129 reward: 19.8\n",
      "Episode 3139 reward: 19.8\n",
      "Episode 3149 reward: 19.7\n",
      "Episode 3159 reward: 19.7\n",
      "Episode 3169 reward: 21.2\n",
      "Episode 3179 reward: 20.7\n",
      "Episode 3189 reward: 19.6\n",
      "Episode 3199 reward: 19.2\n",
      "Episode 3209 reward: 21.6\n",
      "Episode 3219 reward: 20.3\n",
      "Episode 3229 reward: 20.1\n",
      "Episode 3239 reward: 20.9\n",
      "Episode 3249 reward: 20.3\n",
      "Episode 3259 reward: 21.7\n",
      "Episode 3269 reward: 20.8\n",
      "Episode 3279 reward: 21.4\n",
      "Episode 3289 reward: 20.0\n",
      "Episode 3299 reward: 20.5\n",
      "Episode 3309 reward: 17.9\n",
      "Episode 3319 reward: 21.0\n",
      "Episode 3329 reward: 19.6\n",
      "Episode 3339 reward: 20.8\n",
      "Episode 3349 reward: 21.5\n",
      "Episode 3359 reward: 19.9\n",
      "Episode 3369 reward: 18.4\n",
      "Episode 3379 reward: 21.5\n",
      "Episode 3389 reward: 21.8\n",
      "Episode 3399 reward: 20.5\n",
      "Episode 3409 reward: 21.9\n",
      "Episode 3419 reward: 19.5\n",
      "Episode 3429 reward: 20.6\n",
      "Episode 3439 reward: 21.2\n",
      "Episode 3449 reward: 20.9\n",
      "Episode 3459 reward: 21.7\n",
      "Episode 3469 reward: 21.0\n",
      "Episode 3479 reward: 20.4\n",
      "Episode 3489 reward: 20.5\n",
      "Episode 3499 reward: 17.9\n",
      "Episode 3509 reward: 18.9\n",
      "Episode 3519 reward: 20.4\n",
      "Episode 3529 reward: 22.6\n",
      "Episode 3539 reward: 22.0\n",
      "Episode 3549 reward: 20.9\n",
      "Episode 3559 reward: 19.7\n",
      "Episode 3569 reward: 20.7\n",
      "Episode 3579 reward: 20.8\n",
      "Episode 3589 reward: 20.6\n",
      "Episode 3599 reward: 20.3\n",
      "Episode 3609 reward: 22.1\n",
      "Episode 3619 reward: 20.7\n",
      "Episode 3629 reward: 20.7\n",
      "Episode 3639 reward: 18.9\n",
      "Episode 3649 reward: 20.7\n",
      "Episode 3659 reward: 20.4\n",
      "Episode 3669 reward: 18.9\n",
      "Episode 3679 reward: 20.0\n",
      "Episode 3689 reward: 22.1\n",
      "Episode 3699 reward: 20.4\n",
      "Episode 3709 reward: 20.2\n",
      "Episode 3719 reward: 22.0\n",
      "Episode 3729 reward: 19.7\n",
      "Episode 3739 reward: 20.6\n",
      "Episode 3749 reward: 20.7\n",
      "Episode 3759 reward: 19.1\n",
      "Episode 3769 reward: 19.8\n",
      "Episode 3779 reward: 20.2\n",
      "Episode 3789 reward: 19.9\n",
      "Episode 3799 reward: 20.1\n",
      "Episode 3809 reward: 21.3\n",
      "Episode 3819 reward: 18.4\n",
      "Episode 3829 reward: 22.8\n",
      "Episode 3839 reward: 22.3\n",
      "Episode 3849 reward: 21.9\n",
      "Episode 3859 reward: 22.4\n",
      "Episode 3869 reward: 20.6\n",
      "Episode 3879 reward: 20.4\n",
      "Episode 3889 reward: 20.6\n",
      "Episode 3899 reward: 21.0\n",
      "Episode 3909 reward: 20.2\n",
      "Episode 3919 reward: 19.7\n",
      "Episode 3929 reward: 20.0\n",
      "Episode 3939 reward: 21.2\n",
      "Episode 3949 reward: 20.9\n",
      "Episode 3959 reward: 20.8\n",
      "Episode 3969 reward: 20.5\n",
      "Episode 3979 reward: 19.3\n",
      "Episode 3989 reward: 20.4\n",
      "Episode 3999 reward: 19.9\n",
      "Episode 4009 reward: 18.9\n",
      "Episode 4019 reward: 18.0\n",
      "Episode 4029 reward: 21.8\n",
      "Episode 4039 reward: 21.8\n",
      "Episode 4049 reward: 21.6\n",
      "Episode 4059 reward: 21.9\n",
      "Episode 4069 reward: 21.8\n",
      "Episode 4079 reward: 19.7\n",
      "Episode 4089 reward: 22.3\n",
      "Episode 4099 reward: 21.8\n",
      "Episode 4109 reward: 22.4\n",
      "Episode 4119 reward: 20.9\n",
      "Episode 4129 reward: 20.4\n",
      "Episode 4139 reward: 19.5\n",
      "Episode 4149 reward: 20.2\n",
      "Episode 4159 reward: 21.1\n",
      "Episode 4169 reward: 20.3\n",
      "Episode 4179 reward: 20.6\n",
      "Episode 4189 reward: 19.0\n",
      "Episode 4199 reward: 20.9\n",
      "Episode 4209 reward: 20.2\n",
      "Episode 4219 reward: 19.4\n",
      "Episode 4229 reward: 22.9\n",
      "Episode 4239 reward: 21.4\n",
      "Episode 4249 reward: 21.4\n",
      "Episode 4259 reward: 21.2\n",
      "Episode 4269 reward: 22.0\n",
      "Episode 4279 reward: 21.7\n",
      "Episode 4289 reward: 22.0\n",
      "Episode 4299 reward: 19.8\n",
      "Episode 4309 reward: 18.8\n",
      "Episode 4319 reward: 20.7\n",
      "Episode 4329 reward: 20.0\n",
      "Episode 4339 reward: 21.5\n",
      "Episode 4349 reward: 21.2\n",
      "Episode 4359 reward: 20.5\n",
      "Episode 4369 reward: 19.5\n",
      "Episode 4379 reward: 21.3\n",
      "Episode 4389 reward: 21.5\n",
      "Episode 4399 reward: 19.9\n",
      "Episode 4409 reward: 20.2\n",
      "Episode 4419 reward: 20.8\n",
      "Episode 4429 reward: 20.7\n",
      "Episode 4439 reward: 19.4\n",
      "Episode 4449 reward: 22.2\n",
      "Episode 4459 reward: 22.3\n",
      "Episode 4469 reward: 20.7\n",
      "Episode 4479 reward: 18.4\n",
      "Episode 4489 reward: 22.3\n",
      "Episode 4499 reward: 19.1\n",
      "Episode 4509 reward: 21.6\n",
      "Episode 4519 reward: 21.3\n",
      "Episode 4529 reward: 19.8\n",
      "Episode 4539 reward: 21.5\n",
      "Episode 4549 reward: 21.1\n",
      "Episode 4559 reward: 20.3\n",
      "Episode 4569 reward: 19.3\n",
      "Episode 4579 reward: 20.4\n",
      "Episode 4589 reward: 20.1\n",
      "Episode 4599 reward: 17.9\n",
      "Episode 4609 reward: 21.2\n",
      "Episode 4619 reward: 21.3\n",
      "Episode 4629 reward: 20.2\n",
      "Episode 4639 reward: 19.1\n",
      "Episode 4649 reward: 20.4\n",
      "Episode 4659 reward: 21.2\n",
      "Episode 4669 reward: 19.8\n",
      "Episode 4679 reward: 19.4\n",
      "Episode 4689 reward: 19.9\n",
      "Episode 4699 reward: 18.2\n",
      "Episode 4709 reward: 19.3\n",
      "Episode 4719 reward: 20.0\n",
      "Episode 4729 reward: 21.2\n",
      "Episode 4739 reward: 21.2\n",
      "Episode 4749 reward: 20.5\n",
      "Episode 4759 reward: 21.1\n",
      "Episode 4769 reward: 21.6\n",
      "Episode 4779 reward: 21.6\n",
      "Episode 4789 reward: 20.5\n",
      "Episode 4799 reward: 21.5\n",
      "Episode 4809 reward: 21.5\n",
      "Episode 4819 reward: 21.8\n",
      "Episode 4829 reward: 20.9\n",
      "Episode 4839 reward: 20.7\n",
      "Episode 4849 reward: 21.5\n",
      "Episode 4859 reward: 20.1\n",
      "Episode 4869 reward: 20.1\n",
      "Episode 4879 reward: 21.3\n",
      "Episode 4889 reward: 19.2\n",
      "Episode 4899 reward: 20.5\n",
      "Episode 4909 reward: 18.0\n",
      "Episode 4919 reward: 23.4\n",
      "Episode 4929 reward: 22.8\n",
      "Episode 4939 reward: 18.6\n",
      "Episode 4949 reward: 19.6\n",
      "Episode 4959 reward: 21.5\n",
      "Episode 4969 reward: 21.5\n",
      "Episode 4979 reward: 23.1\n",
      "Episode 4989 reward: 19.5\n",
      "Episode 4999 reward: 21.5\n",
      "Episode 5009 reward: 20.3\n",
      "Episode 5019 reward: 20.6\n",
      "Episode 5029 reward: 21.1\n",
      "Episode 5039 reward: 19.7\n",
      "Episode 5049 reward: 21.2\n",
      "Episode 5059 reward: 20.7\n",
      "Episode 5069 reward: 19.3\n",
      "Episode 5079 reward: 20.1\n",
      "Episode 5089 reward: 20.8\n",
      "Episode 5099 reward: 19.9\n",
      "Episode 5109 reward: 21.7\n",
      "Episode 5119 reward: 20.4\n",
      "Episode 5129 reward: 20.2\n",
      "Episode 5139 reward: 21.0\n",
      "Episode 5149 reward: 20.6\n",
      "Episode 5159 reward: 21.4\n",
      "Episode 5169 reward: 21.0\n",
      "Episode 5179 reward: 20.3\n",
      "Episode 5189 reward: 19.5\n",
      "Episode 5199 reward: 20.9\n",
      "Episode 5209 reward: 22.3\n",
      "Episode 5219 reward: 21.6\n",
      "Episode 5229 reward: 20.2\n",
      "Episode 5239 reward: 20.0\n",
      "Episode 5249 reward: 20.5\n",
      "Episode 5259 reward: 20.4\n",
      "Episode 5269 reward: 19.6\n",
      "Episode 5279 reward: 20.7\n",
      "Episode 5289 reward: 20.9\n",
      "Episode 5299 reward: 20.5\n",
      "Episode 5309 reward: 21.6\n",
      "Episode 5319 reward: 18.2\n",
      "Episode 5329 reward: 20.7\n",
      "Episode 5339 reward: 22.9\n",
      "Episode 5349 reward: 18.4\n",
      "Episode 5359 reward: 22.5\n",
      "Episode 5369 reward: 22.3\n",
      "Episode 5379 reward: 20.4\n",
      "Episode 5389 reward: 21.8\n",
      "Episode 5399 reward: 18.9\n",
      "Episode 5409 reward: 20.0\n",
      "Episode 5419 reward: 23.4\n",
      "Episode 5429 reward: 21.2\n",
      "Episode 5439 reward: 19.1\n",
      "Episode 5449 reward: 20.5\n",
      "Episode 5459 reward: 21.2\n",
      "Episode 5469 reward: 21.2\n",
      "Episode 5479 reward: 20.5\n",
      "Episode 5489 reward: 18.3\n",
      "Episode 5499 reward: 20.6\n",
      "Episode 5509 reward: 20.3\n",
      "Episode 5519 reward: 21.9\n",
      "Episode 5529 reward: 19.9\n",
      "Episode 5539 reward: 21.1\n",
      "Episode 5549 reward: 21.3\n",
      "Episode 5559 reward: 21.0\n",
      "Episode 5569 reward: 21.2\n",
      "Episode 5579 reward: 20.5\n",
      "Episode 5589 reward: 20.3\n",
      "Episode 5599 reward: 21.0\n",
      "Episode 5609 reward: 19.5\n",
      "Episode 5619 reward: 21.2\n",
      "Episode 5629 reward: 21.9\n",
      "Episode 5639 reward: 20.4\n",
      "Episode 5649 reward: 20.0\n",
      "Episode 5659 reward: 19.5\n",
      "Episode 5669 reward: 21.9\n",
      "Episode 5679 reward: 22.9\n",
      "Episode 5689 reward: 22.6\n",
      "Episode 5699 reward: 22.0\n",
      "Episode 5709 reward: 21.3\n",
      "Episode 5719 reward: 21.7\n",
      "Episode 5729 reward: 20.5\n",
      "Episode 5739 reward: 20.9\n",
      "Episode 5749 reward: 20.9\n",
      "Episode 5759 reward: 21.5\n",
      "Episode 5769 reward: 20.2\n",
      "Episode 5779 reward: 19.6\n",
      "Episode 5789 reward: 21.2\n",
      "Episode 5799 reward: 19.5\n",
      "Episode 5809 reward: 23.2\n",
      "Episode 5819 reward: 19.9\n",
      "Episode 5829 reward: 20.4\n",
      "Episode 5839 reward: 21.3\n",
      "Episode 5849 reward: 19.8\n",
      "Episode 5859 reward: 21.0\n",
      "Episode 5869 reward: 21.5\n",
      "Episode 5879 reward: 22.0\n",
      "Episode 5889 reward: 20.5\n",
      "Episode 5899 reward: 17.9\n",
      "Episode 5909 reward: 21.9\n",
      "Episode 5919 reward: 20.1\n",
      "Episode 5929 reward: 20.6\n",
      "Episode 5939 reward: 19.9\n",
      "Episode 5949 reward: 20.7\n",
      "Episode 5959 reward: 22.7\n",
      "Episode 5969 reward: 20.9\n",
      "Episode 5979 reward: 21.3\n",
      "Episode 5989 reward: 22.1\n",
      "Episode 5999 reward: 21.9\n",
      "Episode 6009 reward: 21.6\n",
      "Episode 6019 reward: 20.4\n",
      "Episode 6029 reward: 21.2\n",
      "Episode 6039 reward: 19.8\n",
      "Episode 6049 reward: 21.9\n",
      "Episode 6059 reward: 20.8\n",
      "Episode 6069 reward: 22.2\n",
      "Episode 6079 reward: 22.3\n",
      "Episode 6089 reward: 20.2\n",
      "Episode 6099 reward: 19.6\n",
      "Episode 6109 reward: 20.0\n",
      "Episode 6119 reward: 22.3\n",
      "Episode 6129 reward: 19.3\n",
      "Episode 6139 reward: 20.9\n",
      "Episode 6149 reward: 20.7\n",
      "Episode 6159 reward: 19.8\n",
      "Episode 6169 reward: 18.7\n",
      "Episode 6179 reward: 20.6\n",
      "Episode 6189 reward: 21.5\n",
      "Episode 6199 reward: 20.8\n",
      "Episode 6209 reward: 22.0\n",
      "Episode 6219 reward: 21.4\n",
      "Episode 6229 reward: 21.3\n",
      "Episode 6239 reward: 21.8\n",
      "Episode 6249 reward: 19.3\n",
      "Episode 6259 reward: 20.4\n",
      "Episode 6269 reward: 20.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6279 reward: 21.2\n",
      "Episode 6289 reward: 21.5\n",
      "Episode 6299 reward: 20.8\n",
      "Episode 6309 reward: 19.6\n",
      "Episode 6319 reward: 21.3\n",
      "Episode 6329 reward: 20.7\n",
      "Episode 6339 reward: 19.6\n",
      "Episode 6349 reward: 20.7\n",
      "Episode 6359 reward: 21.6\n",
      "Episode 6369 reward: 22.3\n",
      "Episode 6379 reward: 20.6\n",
      "Episode 6389 reward: 20.5\n",
      "Episode 6399 reward: 21.7\n",
      "Episode 6409 reward: 20.6\n",
      "Episode 6419 reward: 20.0\n",
      "Episode 6429 reward: 19.2\n",
      "Episode 6439 reward: 20.0\n",
      "Episode 6449 reward: 20.7\n",
      "Episode 6459 reward: 22.7\n",
      "Episode 6469 reward: 18.0\n",
      "Episode 6479 reward: 20.3\n",
      "Episode 6489 reward: 22.5\n",
      "Episode 6499 reward: 19.2\n",
      "Episode 6509 reward: 19.6\n",
      "Episode 6519 reward: 20.5\n",
      "Episode 6529 reward: 19.7\n",
      "Episode 6539 reward: 23.4\n",
      "Episode 6549 reward: 21.3\n",
      "Episode 6559 reward: 19.9\n",
      "Episode 6569 reward: 22.1\n",
      "Episode 6579 reward: 20.1\n",
      "Episode 6589 reward: 21.7\n",
      "Episode 6599 reward: 21.2\n",
      "Episode 6609 reward: 18.5\n",
      "Episode 6619 reward: 20.8\n",
      "Episode 6629 reward: 20.4\n",
      "Episode 6639 reward: 20.6\n",
      "Episode 6649 reward: 21.4\n",
      "Episode 6659 reward: 19.1\n",
      "Episode 6669 reward: 19.3\n",
      "Episode 6679 reward: 20.5\n",
      "Episode 6689 reward: 22.2\n",
      "Episode 6699 reward: 21.5\n",
      "Episode 6709 reward: 20.6\n",
      "Episode 6719 reward: 19.7\n",
      "Episode 6729 reward: 22.4\n",
      "Episode 6739 reward: 20.7\n",
      "Episode 6749 reward: 22.4\n",
      "Episode 6759 reward: 22.1\n",
      "Episode 6769 reward: 20.2\n",
      "Episode 6779 reward: 21.1\n",
      "Episode 6789 reward: 20.4\n",
      "Episode 6799 reward: 19.9\n",
      "Episode 6809 reward: 20.2\n",
      "Episode 6819 reward: 20.2\n",
      "Episode 6829 reward: 21.3\n",
      "Episode 6839 reward: 19.6\n",
      "Episode 6849 reward: 19.7\n",
      "Episode 6859 reward: 18.6\n",
      "Episode 6869 reward: 19.3\n",
      "Episode 6879 reward: 19.3\n",
      "Episode 6889 reward: 21.7\n",
      "Episode 6899 reward: 20.2\n",
      "Episode 6909 reward: 21.2\n",
      "Episode 6919 reward: 20.6\n",
      "Episode 6929 reward: 21.9\n",
      "Episode 6939 reward: 19.2\n",
      "Episode 6949 reward: 20.7\n",
      "Episode 6959 reward: 19.7\n",
      "Episode 6969 reward: 20.1\n",
      "Episode 6979 reward: 19.8\n",
      "Episode 6989 reward: 20.0\n",
      "Episode 6999 reward: 20.8\n",
      "Episode 7009 reward: 21.1\n",
      "Episode 7019 reward: 20.2\n",
      "Episode 7029 reward: 20.2\n",
      "Episode 7039 reward: 21.5\n",
      "Episode 7049 reward: 21.5\n",
      "Episode 7059 reward: 20.9\n",
      "Episode 7069 reward: 22.2\n",
      "Episode 7079 reward: 22.7\n",
      "Episode 7089 reward: 21.0\n",
      "Episode 7099 reward: 19.4\n",
      "Episode 7109 reward: 19.5\n",
      "Episode 7119 reward: 22.6\n",
      "Episode 7129 reward: 19.7\n",
      "Episode 7139 reward: 19.7\n",
      "Episode 7149 reward: 20.0\n",
      "Episode 7159 reward: 20.2\n",
      "Episode 7169 reward: 20.2\n",
      "Episode 7179 reward: 21.5\n",
      "Episode 7189 reward: 21.2\n",
      "Episode 7199 reward: 20.7\n",
      "Episode 7209 reward: 21.1\n",
      "Episode 7219 reward: 21.1\n",
      "Episode 7229 reward: 20.8\n",
      "Episode 7239 reward: 20.1\n",
      "Episode 7249 reward: 22.3\n",
      "Episode 7259 reward: 20.3\n",
      "Episode 7269 reward: 18.1\n",
      "Episode 7279 reward: 19.8\n",
      "Episode 7289 reward: 20.5\n",
      "Episode 7299 reward: 19.2\n",
      "Episode 7309 reward: 20.4\n",
      "Episode 7319 reward: 20.7\n",
      "Episode 7329 reward: 20.9\n",
      "Episode 7339 reward: 21.0\n",
      "Episode 7349 reward: 19.4\n",
      "Episode 7359 reward: 21.3\n",
      "Episode 7369 reward: 21.6\n",
      "Episode 7379 reward: 21.8\n",
      "Episode 7389 reward: 19.5\n",
      "Episode 7399 reward: 22.0\n",
      "Episode 7409 reward: 21.3\n",
      "Episode 7419 reward: 20.2\n",
      "Episode 7429 reward: 21.3\n",
      "Episode 7439 reward: 21.4\n",
      "Episode 7449 reward: 21.7\n",
      "Episode 7459 reward: 20.5\n",
      "Episode 7469 reward: 20.9\n",
      "Episode 7479 reward: 22.3\n",
      "Episode 7489 reward: 21.1\n",
      "Episode 7499 reward: 21.8\n",
      "Episode 7509 reward: 20.5\n",
      "Episode 7519 reward: 21.8\n",
      "Episode 7529 reward: 21.7\n",
      "Episode 7539 reward: 20.6\n",
      "Episode 7549 reward: 21.5\n",
      "Episode 7559 reward: 20.5\n",
      "Episode 7569 reward: 21.5\n",
      "Episode 7579 reward: 18.9\n",
      "Episode 7589 reward: 20.1\n",
      "Episode 7599 reward: 21.5\n",
      "Episode 7609 reward: 19.9\n",
      "Episode 7619 reward: 18.8\n",
      "Episode 7629 reward: 21.0\n",
      "Episode 7639 reward: 21.3\n",
      "Episode 7649 reward: 19.7\n",
      "Episode 7659 reward: 21.1\n",
      "Episode 7669 reward: 22.2\n",
      "Episode 7679 reward: 20.5\n",
      "Episode 7689 reward: 20.5\n",
      "Episode 7699 reward: 21.5\n",
      "Episode 7709 reward: 20.3\n",
      "Episode 7719 reward: 18.5\n",
      "Episode 7729 reward: 20.1\n",
      "Episode 7739 reward: 21.7\n",
      "Episode 7749 reward: 21.0\n",
      "Episode 7759 reward: 20.8\n",
      "Episode 7769 reward: 20.7\n",
      "Episode 7779 reward: 19.9\n",
      "Episode 7789 reward: 20.4\n",
      "Episode 7799 reward: 22.1\n",
      "Episode 7809 reward: 21.8\n",
      "Episode 7819 reward: 20.0\n",
      "Episode 7829 reward: 20.7\n",
      "Episode 7839 reward: 20.8\n",
      "Episode 7849 reward: 20.8\n",
      "Episode 7859 reward: 20.2\n",
      "Episode 7869 reward: 20.9\n",
      "Episode 7879 reward: 20.7\n",
      "Episode 7889 reward: 20.2\n",
      "Episode 7899 reward: 20.6\n",
      "Episode 7909 reward: 20.8\n",
      "Episode 7919 reward: 20.8\n",
      "Episode 7929 reward: 19.0\n",
      "Episode 7939 reward: 19.8\n",
      "Episode 7949 reward: 22.9\n",
      "Episode 7959 reward: 22.5\n",
      "Episode 7969 reward: 20.2\n",
      "Episode 7979 reward: 21.5\n",
      "Episode 7989 reward: 20.9\n",
      "Episode 7999 reward: 20.6\n",
      "Episode 8009 reward: 20.1\n",
      "Episode 8019 reward: 21.1\n",
      "Episode 8029 reward: 20.2\n",
      "Episode 8039 reward: 20.1\n",
      "Episode 8049 reward: 22.8\n",
      "Episode 8059 reward: 20.1\n",
      "Episode 8069 reward: 21.7\n",
      "Episode 8079 reward: 21.6\n",
      "Episode 8089 reward: 21.3\n",
      "Episode 8099 reward: 21.0\n",
      "Episode 8109 reward: 20.0\n",
      "Episode 8119 reward: 20.5\n",
      "Episode 8129 reward: 22.0\n",
      "Episode 8139 reward: 22.2\n",
      "Episode 8149 reward: 21.7\n",
      "Episode 8159 reward: 17.8\n",
      "Episode 8169 reward: 20.4\n",
      "Episode 8179 reward: 21.7\n",
      "Episode 8189 reward: 20.0\n",
      "Episode 8199 reward: 21.0\n",
      "Episode 8209 reward: 20.4\n",
      "Episode 8219 reward: 19.6\n",
      "Episode 8229 reward: 19.9\n",
      "Episode 8239 reward: 19.0\n",
      "Episode 8249 reward: 20.8\n",
      "Episode 8259 reward: 20.4\n",
      "Episode 8269 reward: 20.3\n",
      "Episode 8279 reward: 22.0\n",
      "Episode 8289 reward: 23.6\n",
      "Episode 8299 reward: 19.0\n",
      "Episode 8309 reward: 19.7\n",
      "Episode 8319 reward: 20.0\n",
      "Episode 8329 reward: 20.3\n",
      "Episode 8339 reward: 20.0\n",
      "Episode 8349 reward: 20.4\n",
      "Episode 8359 reward: 20.3\n",
      "Episode 8369 reward: 20.3\n",
      "Episode 8379 reward: 21.4\n",
      "Episode 8389 reward: 19.5\n",
      "Episode 8399 reward: 19.9\n",
      "Episode 8409 reward: 20.1\n",
      "Episode 8419 reward: 20.8\n",
      "Episode 8429 reward: 21.0\n",
      "Episode 8439 reward: 20.1\n",
      "Episode 8449 reward: 22.3\n",
      "Episode 8459 reward: 20.6\n",
      "Episode 8469 reward: 18.9\n",
      "Episode 8479 reward: 20.6\n",
      "Episode 8489 reward: 21.8\n",
      "Episode 8499 reward: 21.8\n",
      "Episode 8509 reward: 20.3\n",
      "Episode 8519 reward: 20.8\n",
      "Episode 8529 reward: 19.8\n",
      "Episode 8539 reward: 21.0\n",
      "Episode 8549 reward: 18.3\n",
      "Episode 8559 reward: 21.3\n",
      "Episode 8569 reward: 20.1\n",
      "Episode 8579 reward: 18.7\n",
      "Episode 8589 reward: 19.1\n",
      "Episode 8599 reward: 19.6\n",
      "Episode 8609 reward: 20.5\n",
      "Episode 8619 reward: 20.2\n",
      "Episode 8629 reward: 21.2\n",
      "Episode 8639 reward: 21.9\n",
      "Episode 8649 reward: 20.1\n",
      "Episode 8659 reward: 20.5\n",
      "Episode 8669 reward: 19.6\n",
      "Episode 8679 reward: 20.1\n",
      "Episode 8689 reward: 20.5\n",
      "Episode 8699 reward: 20.7\n",
      "Episode 8709 reward: 22.4\n",
      "Episode 8719 reward: 18.9\n",
      "Episode 8729 reward: 19.3\n",
      "Episode 8739 reward: 21.3\n",
      "Episode 8749 reward: 19.4\n",
      "Episode 8759 reward: 22.1\n",
      "Episode 8769 reward: 22.0\n",
      "Episode 8779 reward: 20.3\n",
      "Episode 8789 reward: 19.8\n",
      "Episode 8799 reward: 20.4\n",
      "Episode 8809 reward: 19.7\n",
      "Episode 8819 reward: 19.2\n",
      "Episode 8829 reward: 20.6\n",
      "Episode 8839 reward: 22.1\n",
      "Episode 8849 reward: 20.7\n",
      "Episode 8859 reward: 17.8\n",
      "Episode 8869 reward: 21.7\n",
      "Episode 8879 reward: 20.8\n",
      "Episode 8889 reward: 21.4\n",
      "Episode 8899 reward: 21.7\n",
      "Episode 8909 reward: 20.4\n",
      "Episode 8919 reward: 21.8\n",
      "Episode 8929 reward: 21.3\n",
      "Episode 8939 reward: 19.9\n",
      "Episode 8949 reward: 21.2\n",
      "Episode 8959 reward: 19.9\n",
      "Episode 8969 reward: 18.7\n",
      "Episode 8979 reward: 20.2\n",
      "Episode 8989 reward: 20.8\n",
      "Episode 8999 reward: 22.3\n",
      "Episode 9009 reward: 23.1\n",
      "Episode 9019 reward: 21.6\n",
      "Episode 9029 reward: 21.6\n",
      "Episode 9039 reward: 20.0\n",
      "Episode 9049 reward: 17.9\n",
      "Episode 9059 reward: 19.5\n",
      "Episode 9069 reward: 21.7\n",
      "Episode 9079 reward: 20.4\n",
      "Episode 9089 reward: 19.8\n",
      "Episode 9099 reward: 22.0\n",
      "Episode 9109 reward: 20.3\n",
      "Episode 9119 reward: 21.2\n",
      "Episode 9129 reward: 21.0\n",
      "Episode 9139 reward: 18.9\n",
      "Episode 9149 reward: 19.1\n",
      "Episode 9159 reward: 20.1\n",
      "Episode 9169 reward: 23.1\n",
      "Episode 9179 reward: 21.7\n",
      "Episode 9189 reward: 19.2\n",
      "Episode 9199 reward: 21.6\n",
      "Episode 9209 reward: 19.8\n",
      "Episode 9219 reward: 20.9\n",
      "Episode 9229 reward: 21.6\n",
      "Episode 9239 reward: 21.7\n",
      "Episode 9249 reward: 21.6\n",
      "Episode 9259 reward: 19.9\n",
      "Episode 9269 reward: 20.4\n",
      "Episode 9279 reward: 20.0\n",
      "Episode 9289 reward: 19.6\n",
      "Episode 9299 reward: 19.3\n",
      "Episode 9309 reward: 19.4\n",
      "Episode 9319 reward: 21.0\n",
      "Episode 9329 reward: 22.6\n",
      "Episode 9339 reward: 19.6\n",
      "Episode 9349 reward: 22.7\n",
      "Episode 9359 reward: 20.3\n",
      "Episode 9369 reward: 19.3\n",
      "Episode 9379 reward: 20.8\n",
      "Episode 9389 reward: 20.6\n",
      "Episode 9399 reward: 22.6\n",
      "Episode 9409 reward: 21.7\n",
      "Episode 9419 reward: 19.5\n",
      "Episode 9429 reward: 20.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9439 reward: 19.9\n",
      "Episode 9449 reward: 19.8\n",
      "Episode 9459 reward: 21.0\n",
      "Episode 9469 reward: 19.7\n",
      "Episode 9479 reward: 19.2\n",
      "Episode 9489 reward: 21.1\n",
      "Episode 9499 reward: 20.4\n",
      "Episode 9509 reward: 19.6\n",
      "Episode 9519 reward: 22.2\n",
      "Episode 9529 reward: 22.1\n",
      "Episode 9539 reward: 19.7\n",
      "Episode 9549 reward: 21.6\n",
      "Episode 9559 reward: 20.9\n",
      "Episode 9569 reward: 21.9\n",
      "Episode 9579 reward: 21.0\n",
      "Episode 9589 reward: 20.4\n",
      "Episode 9599 reward: 19.7\n",
      "Episode 9609 reward: 19.7\n",
      "Episode 9619 reward: 20.3\n",
      "Episode 9629 reward: 22.3\n",
      "Episode 9639 reward: 19.9\n",
      "Episode 9649 reward: 19.8\n",
      "Episode 9659 reward: 20.2\n",
      "Episode 9669 reward: 21.6\n",
      "Episode 9679 reward: 22.2\n",
      "Episode 9689 reward: 20.3\n",
      "Episode 9699 reward: 20.0\n",
      "Episode 9709 reward: 20.9\n",
      "Episode 9719 reward: 21.9\n",
      "Episode 9729 reward: 22.7\n",
      "Episode 9739 reward: 21.3\n",
      "Episode 9749 reward: 21.3\n",
      "Episode 9759 reward: 20.9\n",
      "Episode 9769 reward: 20.9\n",
      "Episode 9779 reward: 20.5\n",
      "Episode 9789 reward: 20.6\n",
      "Episode 9799 reward: 20.1\n",
      "Episode 9809 reward: 22.3\n",
      "Episode 9819 reward: 21.1\n",
      "Episode 9829 reward: 19.9\n",
      "Episode 9839 reward: 21.2\n",
      "Episode 9849 reward: 20.4\n",
      "Episode 9859 reward: 22.0\n",
      "Episode 9869 reward: 21.2\n",
      "Episode 9879 reward: 21.5\n",
      "Episode 9889 reward: 19.9\n",
      "Episode 9899 reward: 20.0\n",
      "Episode 9909 reward: 22.9\n",
      "Episode 9919 reward: 19.8\n",
      "Episode 9929 reward: 20.3\n",
      "Episode 9939 reward: 22.4\n",
      "Episode 9949 reward: 20.2\n",
      "Episode 9959 reward: 19.4\n",
      "Episode 9969 reward: 22.1\n",
      "Episode 9979 reward: 22.4\n",
      "Episode 9989 reward: 20.2\n",
      "Episode 9999 reward: 22.7\n",
      "Mean reward per episode: 19.6449\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size,cell,'main')\n",
    "targetQN = Qnetwork(h_size,cellT,'target')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "   #updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        state = (np.zeros([1, h_size]), np.zeros([1, h_size]))\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: \n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                state = sess.run(mainQN.lstm_state, feed_dict={mainQN.scalarInput: [s],\n",
    "                                                              mainQN.lstm_input_size: 1,\n",
    "                                                              mainQN.lstm_state_in: state,\n",
    "                                                              mainQN.batch_size: 1})\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a, state = sess.run([mainQN.predict, mainQN.lstm_state], feed_dict={mainQN.scalarInput: [s],\n",
    "                                                                                   mainQN.lstm_input_size: 1,\n",
    "                                                                                   mainQN.lstm_state_in: state,\n",
    "                                                                                   mainQN.batch_size: 1})\n",
    "                a = a[0]\n",
    "            total_steps += 1\n",
    "            \n",
    "            ###############################################################################\n",
    "            # TODO: Save the experience to our episode buffer.                             #\n",
    "            # You will need to do the following:                                           #\n",
    "            # (1) Get new state s1 (resized), reward r and done d from a                   #\n",
    "            # (2) Add experience to episode buffer. Hint: experience includes              #\n",
    "            #     s, a, r, s1 and d.                                                       #\n",
    "            ################################################################################\n",
    "            s1, r, d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            experience = np.expand_dims(np.array([s, a, r, s1, d]), 0)\n",
    "            episodeBuffer.add(experience)   \n",
    "            ################################################################################\n",
    "            #                                 END OF YOUR CODE                             #\n",
    "            ################################################################################\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO: Implement Double-DQN                                                   #\n",
    "                    # (1) Get a random batch of experiences via experience_buffer class            #\n",
    "                    #                                                                              #\n",
    "                    # (2) Perform the Double-DQN update to the target Q-values                     #\n",
    "                    #     Hint: Use mainQN and targetQN separately to chose an action and predict  #\n",
    "                    #     the Q-values for that action.                                            #\n",
    "                    #     Then compute targetQ based on Double-DQN equation                        #\n",
    "                    #                                                                              #\n",
    "                    # (3) Update the primary network with our target values                        #\n",
    "                    ################################################################################ \n",
    "                    initial_state = (np.zeros([batch_size,h_size]), np.zeros([batch_size,h_size])) \n",
    "                    \n",
    "                    batch = myBuffer.sample(batch_size, trace_length)\n",
    "                    stacked_state = np.vstack(batch[:, 3])\n",
    "\n",
    "                    action_ = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput: stacked_state,\n",
    "                                                                  mainQN.lstm_input_size: trace_length,\n",
    "                                                                  mainQN.lstm_state_in: initial_state,\n",
    "                                                                  mainQN.batch_size: batch_size})\n",
    "                    Q_ = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput: stacked_state,\n",
    "                                                           targetQN.lstm_input_size: trace_length,\n",
    "                                                           targetQN.lstm_state_in: initial_state,\n",
    "                                                           targetQN.batch_size: batch_size})\n",
    "                    next_Q = Q_[range(batch_size*trace_length), action_]\n",
    "                    done_mask = 1 - batch[:, 4]\n",
    "                    targetQ = batch[:, 2] + done_mask * y * next_Q\n",
    "                    \n",
    "                    sess.run(mainQN.updateModel, feed_dict={mainQN.scalarInput: np.vstack(batch[:,0]),\n",
    "                                                            mainQN.targetQ: targetQ,\n",
    "                                                            mainQN.actions: batch[:,1],\n",
    "                                                            mainQN.lstm_input_size:trace_length,\n",
    "                                                            mainQN.lstm_state_in: initial_state,\n",
    "                                                            mainQN.batch_size: batch_size})\n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "                    \n",
    "                    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n",
    "\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "\n",
    "        #Add the episode to the experience buffer\n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(\"Episode\",i,\"reward:\",np.mean(rList[-10:]))\n",
    "print(\"Mean reward per episode: \" + str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking network learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14763e30cf28>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnVkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x147695cf7f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_2018u2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
