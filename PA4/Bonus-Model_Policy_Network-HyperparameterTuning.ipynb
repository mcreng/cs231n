{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based RL - Tuned\n",
    "In this exercise you will implement a policy and model network which work in tandem to solve the CartPole reinforcement learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bonus task where this Model Policy Network is tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries and starting CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info.major > 2:\n",
    "    xrange = range\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u20842/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 16 # number of hidden layer neurons\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "\n",
    "model_bs = 3 # Batch size when learning from model\n",
    "real_bs = 3 # Batch size when learning from real environment\n",
    "\n",
    "# model initialization\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "observations = tf.placeholder(tf.float32, [None,4] , name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[4, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "\n",
    "################################################################################\n",
    "# TODO: Implement the loss function.                                           #\n",
    "# This sends the weights in the direction of making actions that gave good     #\n",
    "# advantage (reward overtime) more likely, and actions that didn't less likely.#\n",
    "################################################################################\n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages) \n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Network\n",
    "Here we implement a multi-layer neural network that predicts the next observation, reward, and done state from a current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mH = 256 # model layer size\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, [None, 5])\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [mH, 50])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [50])\n",
    "\n",
    "previous_state = tf.placeholder(tf.float32, [None,5] , name=\"previous_state\")\n",
    "W1M = tf.get_variable(\"W1M\", shape=[5, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1M = tf.Variable(tf.zeros([mH]),name=\"B1M\")\n",
    "layer1M = tf.nn.relu(tf.matmul(previous_state,W1M) + B1M)\n",
    "W2M = tf.get_variable(\"W2M\", shape=[mH, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2M = tf.Variable(tf.zeros([mH]),name=\"B2M\")\n",
    "layer2M = tf.nn.relu(tf.matmul(layer1M,W2M) + B2M)\n",
    "wO = tf.get_variable(\"wO\", shape=[mH, 4],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wR = tf.get_variable(\"wR\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wD = tf.get_variable(\"wD\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "bO = tf.Variable(tf.zeros([4]),name=\"bO\")\n",
    "bR = tf.Variable(tf.zeros([1]),name=\"bR\")\n",
    "bD = tf.Variable(tf.ones([1]),name=\"bD\")\n",
    "\n",
    "\n",
    "predicted_observation = tf.matmul(layer2M,wO,name=\"predicted_observation\") + bO\n",
    "predicted_reward = tf.matmul(layer2M,wR,name=\"predicted_reward\") + bR\n",
    "predicted_done = tf.sigmoid(tf.matmul(layer2M,wD,name=\"predicted_done\") + bD)\n",
    "\n",
    "true_observation = tf.placeholder(tf.float32,[None,4],name=\"true_observation\")\n",
    "true_reward = tf.placeholder(tf.float32,[None,1],name=\"true_reward\")\n",
    "true_done = tf.placeholder(tf.float32,[None,1],name=\"true_done\")\n",
    "\n",
    "\n",
    "predicted_state = tf.concat([predicted_observation,predicted_reward,predicted_done],1)\n",
    "\n",
    "observation_loss = tf.square(true_observation - predicted_observation)\n",
    "\n",
    "reward_loss = tf.square(true_reward - predicted_reward)\n",
    "\n",
    "done_loss = tf.multiply(predicted_done, true_done) + tf.multiply(1-predicted_done, 1-true_done)\n",
    "done_loss = -tf.log(done_loss)\n",
    "\n",
    "model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)\n",
    "\n",
    "modelAdam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "updateModel = modelAdam.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "\n",
    "def discount_rewards(r):\n",
    "    ################################################################################\n",
    "    # TODO: Implement the discounted rewards function                              #\n",
    "    # Return discounted rewards weighed by gamma. Each reward will be replaced     #\n",
    "    # with a weight reward that involves itself and all the other rewards occuring #\n",
    "    # after it. The later the reward after it happens, the less effect it has on   #\n",
    "    # the current rewards's discounted reward                                      #\n",
    "    # Hint: [r0, r1, r2, ..., r_N] will look someting like:                        #\n",
    "    #       [(r0 + r1*gamma^1 + ... r_N*gamma^N), (r1 + r2*gamma^1 + ...), ...]    #\n",
    "    ################################################################################\n",
    "    rnew = np.copy(r)\n",
    "    for i in range(1, len(rnew)):\n",
    "        rnew[:len(r)-i] += gamma**i * r[i:]\n",
    "    return rnew        \n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################\n",
    "\n",
    "# This function uses our model to produce a new state when given a previous state and action\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0],np.array(action)]),[1,5])\n",
    "    myPredict = sess.run([predicted_state],feed_dict={previous_state: toFeed})\n",
    "    reward = myPredict[0][:,4]\n",
    "    observation = myPredict[0][:,0:4]\n",
    "    observation[:,0] = np.clip(observation[:,0],-2.4,2.4)\n",
    "    observation[:,2] = np.clip(observation[:,2],-0.4,0.4)\n",
    "    doneP = np.clip(myPredict[0][:,5],0,1)\n",
    "    if doneP > 0.1 or len(xs)>= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Policy and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 4.000000. Reward 19.000000. action: 0.000000. mean reward 19.000000.\n",
      "World Perf: Episode 7.000000. Reward 25.333333. action: 1.000000. mean reward 19.063333.\n",
      "World Perf: Episode 10.000000. Reward 22.666667. action: 1.000000. mean reward 19.099367.\n",
      "World Perf: Episode 13.000000. Reward 27.000000. action: 0.000000. mean reward 19.178373.\n",
      "World Perf: Episode 16.000000. Reward 19.666667. action: 1.000000. mean reward 19.183256.\n",
      "World Perf: Episode 19.000000. Reward 19.333333. action: 0.000000. mean reward 19.184757.\n",
      "World Perf: Episode 22.000000. Reward 25.666667. action: 1.000000. mean reward 19.249576.\n",
      "World Perf: Episode 25.000000. Reward 20.000000. action: 0.000000. mean reward 19.257080.\n",
      "World Perf: Episode 28.000000. Reward 17.333333. action: 0.000000. mean reward 19.237843.\n",
      "World Perf: Episode 31.000000. Reward 19.666667. action: 0.000000. mean reward 19.242131.\n",
      "World Perf: Episode 34.000000. Reward 19.333333. action: 1.000000. mean reward 19.243043.\n",
      "World Perf: Episode 37.000000. Reward 24.000000. action: 1.000000. mean reward 19.290612.\n",
      "World Perf: Episode 40.000000. Reward 40.333333. action: 0.000000. mean reward 19.501040.\n",
      "World Perf: Episode 43.000000. Reward 21.666667. action: 0.000000. mean reward 19.522696.\n",
      "World Perf: Episode 46.000000. Reward 23.666667. action: 0.000000. mean reward 19.564136.\n",
      "World Perf: Episode 49.000000. Reward 17.333333. action: 0.000000. mean reward 19.541828.\n",
      "World Perf: Episode 52.000000. Reward 27.666667. action: 0.000000. mean reward 19.623076.\n",
      "World Perf: Episode 55.000000. Reward 21.333333. action: 1.000000. mean reward 19.640179.\n",
      "World Perf: Episode 58.000000. Reward 20.666667. action: 1.000000. mean reward 19.650443.\n",
      "World Perf: Episode 61.000000. Reward 16.666667. action: 0.000000. mean reward 19.620606.\n",
      "World Perf: Episode 64.000000. Reward 19.666667. action: 0.000000. mean reward 19.621066.\n",
      "World Perf: Episode 67.000000. Reward 21.666667. action: 1.000000. mean reward 19.641522.\n",
      "World Perf: Episode 70.000000. Reward 32.333333. action: 0.000000. mean reward 19.768440.\n",
      "World Perf: Episode 73.000000. Reward 27.333333. action: 0.000000. mean reward 19.844089.\n",
      "World Perf: Episode 76.000000. Reward 34.333333. action: 1.000000. mean reward 19.988982.\n",
      "World Perf: Episode 79.000000. Reward 20.333333. action: 1.000000. mean reward 19.992425.\n",
      "World Perf: Episode 82.000000. Reward 20.000000. action: 1.000000. mean reward 19.992501.\n",
      "World Perf: Episode 85.000000. Reward 23.333333. action: 1.000000. mean reward 20.025909.\n",
      "World Perf: Episode 88.000000. Reward 35.000000. action: 1.000000. mean reward 20.175650.\n",
      "World Perf: Episode 91.000000. Reward 20.333333. action: 0.000000. mean reward 20.177227.\n",
      "World Perf: Episode 94.000000. Reward 35.000000. action: 1.000000. mean reward 20.325455.\n",
      "World Perf: Episode 97.000000. Reward 19.666667. action: 0.000000. mean reward 20.318867.\n",
      "World Perf: Episode 100.000000. Reward 12.666667. action: 0.000000. mean reward 20.242345.\n",
      "World Perf: Episode 103.000000. Reward 13.333333. action: 0.000000. mean reward 20.173255.\n",
      "World Perf: Episode 106.000000. Reward 26.666667. action: 1.000000. mean reward 20.160816.\n",
      "World Perf: Episode 109.000000. Reward 23.000000. action: 0.000000. mean reward 20.107731.\n",
      "World Perf: Episode 112.000000. Reward 19.000000. action: 1.000000. mean reward 20.023344.\n",
      "World Perf: Episode 115.000000. Reward 20.666667. action: 0.000000. mean reward 20.020765.\n",
      "World Perf: Episode 118.000000. Reward 25.333333. action: 0.000000. mean reward 19.961702.\n",
      "World Perf: Episode 121.000000. Reward 23.000000. action: 1.000000. mean reward 19.928999.\n",
      "World Perf: Episode 124.000000. Reward 18.666667. action: 0.000000. mean reward 19.872879.\n",
      "World Perf: Episode 127.000000. Reward 17.000000. action: 1.000000. mean reward 19.760763.\n",
      "World Perf: Episode 130.000000. Reward 19.666667. action: 0.000000. mean reward 21.086786.\n",
      "World Perf: Episode 133.000000. Reward 16.000000. action: 0.000000. mean reward 20.955912.\n",
      "World Perf: Episode 136.000000. Reward 27.000000. action: 0.000000. mean reward 20.866470.\n",
      "World Perf: Episode 139.000000. Reward 17.333333. action: 0.000000. mean reward 20.729610.\n",
      "World Perf: Episode 142.000000. Reward 30.333333. action: 0.000000. mean reward 20.679577.\n",
      "World Perf: Episode 145.000000. Reward 25.000000. action: 0.000000. mean reward 20.827681.\n",
      "World Perf: Episode 148.000000. Reward 23.666667. action: 1.000000. mean reward 23.354219.\n",
      "World Perf: Episode 151.000000. Reward 23.333333. action: 0.000000. mean reward 26.192713.\n",
      "World Perf: Episode 154.000000. Reward 30.000000. action: 0.000000. mean reward 26.605247.\n",
      "World Perf: Episode 157.000000. Reward 54.000000. action: 1.000000. mean reward 26.655001.\n",
      "World Perf: Episode 160.000000. Reward 33.000000. action: 0.000000. mean reward 26.577667.\n",
      "World Perf: Episode 163.000000. Reward 29.666667. action: 0.000000. mean reward 26.596146.\n",
      "World Perf: Episode 166.000000. Reward 27.333333. action: 1.000000. mean reward 26.538157.\n",
      "World Perf: Episode 169.000000. Reward 64.333333. action: 1.000000. mean reward 26.696426.\n",
      "World Perf: Episode 172.000000. Reward 43.000000. action: 1.000000. mean reward 27.239616.\n",
      "World Perf: Episode 175.000000. Reward 48.666667. action: 0.000000. mean reward 29.538530.\n",
      "World Perf: Episode 178.000000. Reward 26.000000. action: 1.000000. mean reward 29.289825.\n",
      "World Perf: Episode 181.000000. Reward 38.333333. action: 1.000000. mean reward 29.181463.\n",
      "World Perf: Episode 184.000000. Reward 62.666667. action: 1.000000. mean reward 30.186125.\n",
      "World Perf: Episode 187.000000. Reward 22.666667. action: 1.000000. mean reward 32.867023.\n",
      "World Perf: Episode 190.000000. Reward 63.000000. action: 1.000000. mean reward 32.903473.\n",
      "World Perf: Episode 193.000000. Reward 98.333333. action: 0.000000. mean reward 33.292240.\n",
      "World Perf: Episode 196.000000. Reward 65.333333. action: 1.000000. mean reward 35.173031.\n",
      "World Perf: Episode 199.000000. Reward 41.666667. action: 1.000000. mean reward 38.038906.\n",
      "World Perf: Episode 202.000000. Reward 44.000000. action: 1.000000. mean reward 37.771332.\n",
      "World Perf: Episode 205.000000. Reward 66.666667. action: 1.000000. mean reward 37.779060.\n",
      "World Perf: Episode 208.000000. Reward 60.333333. action: 0.000000. mean reward 37.720272.\n",
      "World Perf: Episode 211.000000. Reward 34.000000. action: 1.000000. mean reward 37.419216.\n",
      "World Perf: Episode 214.000000. Reward 30.000000. action: 0.000000. mean reward 37.620312.\n",
      "World Perf: Episode 217.000000. Reward 46.666667. action: 0.000000. mean reward 37.427105.\n",
      "World Perf: Episode 220.000000. Reward 40.333333. action: 1.000000. mean reward 37.433071.\n",
      "World Perf: Episode 223.000000. Reward 41.333333. action: 1.000000. mean reward 37.371838.\n",
      "World Perf: Episode 226.000000. Reward 43.333333. action: 1.000000. mean reward 37.280632.\n",
      "World Perf: Episode 229.000000. Reward 61.666667. action: 1.000000. mean reward 37.347809.\n",
      "World Perf: Episode 232.000000. Reward 72.000000. action: 1.000000. mean reward 37.598980.\n",
      "World Perf: Episode 235.000000. Reward 77.333333. action: 1.000000. mean reward 40.539330.\n",
      "World Perf: Episode 238.000000. Reward 60.333333. action: 1.000000. mean reward 41.300999.\n",
      "World Perf: Episode 241.000000. Reward 31.333333. action: 1.000000. mean reward 40.893749.\n",
      "World Perf: Episode 244.000000. Reward 42.333333. action: 1.000000. mean reward 41.976902.\n",
      "World Perf: Episode 247.000000. Reward 85.666667. action: 1.000000. mean reward 42.332020.\n",
      "World Perf: Episode 250.000000. Reward 67.333333. action: 1.000000. mean reward 45.062092.\n",
      "World Perf: Episode 253.000000. Reward 78.333333. action: 0.000000. mean reward 47.869877.\n",
      "World Perf: Episode 256.000000. Reward 56.000000. action: 0.000000. mean reward 47.626255.\n",
      "World Perf: Episode 259.000000. Reward 71.333333. action: 0.000000. mean reward 50.488789.\n",
      "World Perf: Episode 262.000000. Reward 92.666667. action: 1.000000. mean reward 50.599895.\n",
      "World Perf: Episode 265.000000. Reward 53.666667. action: 0.000000. mean reward 53.073410.\n",
      "World Perf: Episode 268.000000. Reward 74.666667. action: 0.000000. mean reward 53.037830.\n",
      "World Perf: Episode 271.000000. Reward 85.333333. action: 1.000000. mean reward 55.152660.\n",
      "World Perf: Episode 274.000000. Reward 57.666667. action: 1.000000. mean reward 55.088223.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 277.000000. Reward 88.000000. action: 0.000000. mean reward 55.016239.\n",
      "World Perf: Episode 280.000000. Reward 72.333333. action: 0.000000. mean reward 54.896378.\n",
      "World Perf: Episode 283.000000. Reward 80.000000. action: 1.000000. mean reward 54.836029.\n",
      "World Perf: Episode 286.000000. Reward 55.333333. action: 1.000000. mean reward 54.833191.\n",
      "World Perf: Episode 289.000000. Reward 58.666667. action: 0.000000. mean reward 54.474930.\n",
      "World Perf: Episode 292.000000. Reward 133.000000. action: 1.000000. mean reward 55.695812.\n",
      "World Perf: Episode 295.000000. Reward 115.333333. action: 1.000000. mean reward 58.561275.\n",
      "World Perf: Episode 298.000000. Reward 68.000000. action: 1.000000. mean reward 61.124741.\n",
      "World Perf: Episode 301.000000. Reward 78.000000. action: 0.000000. mean reward 61.314190.\n",
      "World Perf: Episode 304.000000. Reward 100.666667. action: 1.000000. mean reward 64.012245.\n",
      "World Perf: Episode 307.000000. Reward 85.000000. action: 0.000000. mean reward 63.920826.\n",
      "World Perf: Episode 310.000000. Reward 78.333333. action: 1.000000. mean reward 66.407204.\n",
      "World Perf: Episode 313.000000. Reward 75.333333. action: 0.000000. mean reward 66.748856.\n",
      "World Perf: Episode 316.000000. Reward 118.666667. action: 0.000000. mean reward 69.652184.\n",
      "World Perf: Episode 319.000000. Reward 102.666667. action: 0.000000. mean reward 71.664612.\n",
      "World Perf: Episode 322.000000. Reward 87.666667. action: 1.000000. mean reward 71.323273.\n",
      "World Perf: Episode 325.000000. Reward 108.333333. action: 1.000000. mean reward 71.315971.\n",
      "World Perf: Episode 328.000000. Reward 95.333333. action: 0.000000. mean reward 71.056099.\n",
      "World Perf: Episode 331.000000. Reward 134.000000. action: 0.000000. mean reward 71.519737.\n",
      "World Perf: Episode 334.000000. Reward 104.666667. action: 1.000000. mean reward 74.101387.\n",
      "World Perf: Episode 337.000000. Reward 155.000000. action: 0.000000. mean reward 76.804909.\n",
      "World Perf: Episode 340.000000. Reward 122.000000. action: 1.000000. mean reward 76.687141.\n",
      "World Perf: Episode 343.000000. Reward 123.000000. action: 1.000000. mean reward 76.581459.\n",
      "World Perf: Episode 346.000000. Reward 122.333333. action: 1.000000. mean reward 76.873199.\n",
      "World Perf: Episode 349.000000. Reward 144.333333. action: 1.000000. mean reward 79.765541.\n",
      "World Perf: Episode 352.000000. Reward 144.666667. action: 1.000000. mean reward 80.381737.\n",
      "World Perf: Episode 355.000000. Reward 170.333333. action: 1.000000. mean reward 80.724480.\n",
      "World Perf: Episode 358.000000. Reward 159.000000. action: 1.000000. mean reward 83.719460.\n",
      "World Perf: Episode 361.000000. Reward 138.666667. action: 1.000000. mean reward 83.649437.\n",
      "World Perf: Episode 364.000000. Reward 141.333333. action: 0.000000. mean reward 86.585640.\n",
      "World Perf: Episode 367.000000. Reward 140.000000. action: 0.000000. mean reward 86.705566.\n",
      "World Perf: Episode 370.000000. Reward 200.000000. action: 0.000000. mean reward 88.650055.\n",
      "370\n"
     ]
    }
   ],
   "source": [
    "xs,drs,ys,ds = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = real_bs\n",
    "\n",
    "drawFromModel = False # When set to True, will use model for observations\n",
    "trainTheModel = True # Whether to train the model\n",
    "trainThePolicy = False # Whether to train the policy\n",
    "switch_point = 1\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number <= 5000:\n",
    "        # Start displaying environment once performance is acceptably high.\n",
    "        if (reward_sum/batch_size > 150 and drawFromModel == False) or rendering == True : \n",
    "#             env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        x = np.reshape(observation,[1,4])\n",
    "\n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) \n",
    "        y = 1 if action == 0 else 0 \n",
    "        ys.append(y)\n",
    "        \n",
    "        # step the  model or real environment and get new measurements\n",
    "        if drawFromModel == False:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else:\n",
    "            observation, reward, done = stepModel(sess,xs,action)\n",
    "                \n",
    "        reward_sum += reward\n",
    "        \n",
    "        ds.append(done*1)\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            \n",
    "            if drawFromModel == False: \n",
    "                real_episodes += 1\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            xs,drs,ys,ds = [],[],[],[] # reset array memory\n",
    "            \n",
    "            if trainTheModel == True:\n",
    "                \n",
    "                ################################################################################\n",
    "                # TODO: Run the model network and compute predicted_state                      #\n",
    "                # Output: 'pState'                                                             #\n",
    "                ################################################################################\n",
    "                feed_dict = {\n",
    "                    previous_state: np.hstack([epx[:-1], np.array([1-y for y in epy][:-1])]),\n",
    "                    true_observation: epx[1:],\n",
    "                    true_reward: epr[1:],\n",
    "                    true_done: epd[1:]\n",
    "                }\n",
    "                tState = np.hstack([epx[1:], epr[1:], epd[1:]])\n",
    "                _, pState = sess.run([updateModel, predicted_state], feed_dict=feed_dict)\n",
    "                ################################################################################\n",
    "                #                                 END OF YOUR CODE                             #\n",
    "                ################################################################################\n",
    "                \n",
    "\n",
    "            if trainThePolicy == True:\n",
    "                \n",
    "                ################################################################################\n",
    "                # TODO: Run the policy network and compute newGrads                            #\n",
    "                # Output: 'tGrad'                                                              #\n",
    "                ################################################################################\n",
    "                discounted_epr = discount_rewards(epr)\n",
    "                # size the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr //= np.std(discounted_epr)\n",
    "                tGrad = sess.run(newGrads, feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "                ################################################################################\n",
    "                #                                 END OF YOUR CODE                             #\n",
    "                ################################################################################\n",
    "                \n",
    "                # If gradients becom too large, end training process\n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix,grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                \n",
    "            if switch_point + batch_size == episode_number: \n",
    "                switch_point = episode_number\n",
    "                if trainThePolicy == True:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO:                                                                        #\n",
    "                    # (1) Run the policy network and update gradients                              #\n",
    "                    # (2) Reset gradBuffer to 0                                                    #\n",
    "                    ################################################################################\n",
    "                    sess.run(updateGrads, feed_dict={W1Grad: gradBuffer[0], W2Grad: gradBuffer[1]})\n",
    "                    # gradBuffer reset is already done at the beginning of episode\n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                if drawFromModel == False:\n",
    "                    print('World Perf: Episode %f. Reward %f. action: %f. mean reward %f.' % (real_episodes,reward_sum/real_bs,action, running_reward/real_bs))\n",
    "                    if reward_sum/batch_size >= 200:\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "\n",
    "                # Once the model has been trained on 100 episodes\n",
    "                if episode_number > 100:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO: Alternating between training the policy from the model and training    #\n",
    "                    # the model from the real environment.                                         #\n",
    "                    ################################################################################\n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainTheModel = not trainTheModel\n",
    "                    trainThePolicy = not trainThePolicy\n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1,0.1,[4]) # Generate reasonable starting point\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "                \n",
    "print(real_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original model terminates (solves the game) at episode 895; the new model terminates at episode 370, which is much quicker."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_2018u2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
